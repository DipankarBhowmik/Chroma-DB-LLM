{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "185f1717-3cb5-4618-b07c-3f5555bb94b3",
   "metadata": {},
   "source": [
    "# Enhancing Medical Report Findings with Retrieval-Augmented Generation (RAG): Integrating LLM Models and Chroma DB using the LangChain framework for searchable data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e80c4b2d-61d5-4861-81d3-32c5d599923b",
   "metadata": {},
   "source": [
    "### Installing the required libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "697b86b7-0892-4c70-b75a-2aa3d0842f18",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: PyPDF2 in c:\\users\\bhowm\\appdata\\roaming\\python\\python312\\site-packages (3.0.1)\n",
      "Requirement already satisfied: chromadb==0.4.0 in c:\\users\\bhowm\\appdata\\roaming\\python\\python312\\site-packages (0.4.0)\n",
      "Collecting langchain==0.2.0\n",
      "  Using cached langchain-0.2.0-py3-none-any.whl.metadata (13 kB)\n",
      "Collecting langchain-community\n",
      "  Downloading langchain_community-0.3.14-py3-none-any.whl.metadata (2.9 kB)\n",
      "Collecting sentence-transformers\n",
      "  Using cached sentence_transformers-3.3.1-py3-none-any.whl.metadata (10 kB)\n",
      "Requirement already satisfied: pandas>=1.3 in c:\\users\\bhowm\\appdata\\roaming\\python\\python312\\site-packages (from chromadb==0.4.0) (2.2.3)\n",
      "Requirement already satisfied: requests>=2.28 in c:\\users\\bhowm\\.conda\\envs\\new1\\lib\\site-packages (from chromadb==0.4.0) (2.32.3)\n",
      "Collecting pydantic<2.0,>=1.9 (from chromadb==0.4.0)\n",
      "  Using cached pydantic-1.10.19-cp312-cp312-win_amd64.whl.metadata (153 kB)\n",
      "Requirement already satisfied: chroma-hnswlib==0.7.1 in c:\\users\\bhowm\\appdata\\roaming\\python\\python312\\site-packages (from chromadb==0.4.0) (0.7.1)\n",
      "Requirement already satisfied: fastapi<0.100.0,>=0.95.2 in c:\\users\\bhowm\\appdata\\roaming\\python\\python312\\site-packages (from chromadb==0.4.0) (0.99.1)\n",
      "Requirement already satisfied: uvicorn>=0.18.3 in c:\\users\\bhowm\\appdata\\roaming\\python\\python312\\site-packages (from uvicorn[standard]>=0.18.3->chromadb==0.4.0) (0.34.0)\n",
      "Collecting numpy>=1.21.6 (from chromadb==0.4.0)\n",
      "  Downloading numpy-2.2.1-cp312-cp312-win_amd64.whl.metadata (60 kB)\n",
      "Requirement already satisfied: posthog>=2.4.0 in c:\\users\\bhowm\\appdata\\roaming\\python\\python312\\site-packages (from chromadb==0.4.0) (3.7.4)\n",
      "Requirement already satisfied: typing-extensions>=4.5.0 in c:\\users\\bhowm\\.conda\\envs\\new1\\lib\\site-packages (from chromadb==0.4.0) (4.12.2)\n",
      "Requirement already satisfied: pulsar-client>=3.1.0 in c:\\users\\bhowm\\appdata\\roaming\\python\\python312\\site-packages (from chromadb==0.4.0) (3.5.0)\n",
      "Requirement already satisfied: onnxruntime>=1.14.1 in c:\\users\\bhowm\\appdata\\roaming\\python\\python312\\site-packages (from chromadb==0.4.0) (1.20.1)\n",
      "Collecting tokenizers>=0.13.2 (from chromadb==0.4.0)\n",
      "  Using cached tokenizers-0.21.0-cp39-abi3-win_amd64.whl.metadata (6.9 kB)\n",
      "Requirement already satisfied: pypika>=0.48.9 in c:\\users\\bhowm\\appdata\\roaming\\python\\python312\\site-packages (from chromadb==0.4.0) (0.48.9)\n",
      "Collecting tqdm>=4.65.0 (from chromadb==0.4.0)\n",
      "  Using cached tqdm-4.67.1-py3-none-any.whl.metadata (57 kB)\n",
      "Requirement already satisfied: overrides>=7.3.1 in c:\\users\\bhowm\\.conda\\envs\\new1\\lib\\site-packages (from chromadb==0.4.0) (7.4.0)\n",
      "Requirement already satisfied: importlib-resources in c:\\users\\bhowm\\appdata\\roaming\\python\\python312\\site-packages (from chromadb==0.4.0) (6.4.5)\n",
      "Requirement already satisfied: PyYAML>=5.3 in c:\\users\\bhowm\\.conda\\envs\\new1\\lib\\site-packages (from langchain==0.2.0) (6.0.2)\n",
      "Collecting SQLAlchemy<3,>=1.4 (from langchain==0.2.0)\n",
      "  Using cached SQLAlchemy-2.0.36-cp312-cp312-win_amd64.whl.metadata (9.9 kB)\n",
      "Collecting aiohttp<4.0.0,>=3.8.3 (from langchain==0.2.0)\n",
      "  Using cached aiohttp-3.11.11-cp312-cp312-win_amd64.whl.metadata (8.0 kB)\n",
      "Collecting dataclasses-json<0.7,>=0.5.7 (from langchain==0.2.0)\n",
      "  Using cached dataclasses_json-0.6.7-py3-none-any.whl.metadata (25 kB)\n",
      "Collecting langchain-core<0.3.0,>=0.2.0 (from langchain==0.2.0)\n",
      "  Using cached langchain_core-0.2.43-py3-none-any.whl.metadata (6.2 kB)\n",
      "Collecting langchain-text-splitters<0.3.0,>=0.2.0 (from langchain==0.2.0)\n",
      "  Using cached langchain_text_splitters-0.2.4-py3-none-any.whl.metadata (2.3 kB)\n",
      "Collecting langsmith<0.2.0,>=0.1.17 (from langchain==0.2.0)\n",
      "  Using cached langsmith-0.1.147-py3-none-any.whl.metadata (14 kB)\n",
      "Collecting numpy>=1.21.6 (from chromadb==0.4.0)\n",
      "  Using cached numpy-1.26.4-cp312-cp312-win_amd64.whl.metadata (61 kB)\n",
      "Collecting tenacity<9.0.0,>=8.1.0 (from langchain==0.2.0)\n",
      "  Using cached tenacity-8.5.0-py3-none-any.whl.metadata (1.2 kB)\n",
      "Collecting httpx-sse<0.5.0,>=0.4.0 (from langchain-community)\n",
      "  Using cached httpx_sse-0.4.0-py3-none-any.whl.metadata (9.0 kB)\n",
      "INFO: pip is looking at multiple versions of langchain-community to determine which version is compatible with other requirements. This could take a while.\n",
      "Collecting langchain-community\n",
      "  Using cached langchain_community-0.3.13-py3-none-any.whl.metadata (2.9 kB)\n",
      "  Using cached langchain_community-0.3.12-py3-none-any.whl.metadata (2.9 kB)\n",
      "  Using cached langchain_community-0.3.11-py3-none-any.whl.metadata (2.9 kB)\n",
      "  Using cached langchain_community-0.3.10-py3-none-any.whl.metadata (2.9 kB)\n",
      "  Using cached langchain_community-0.3.9-py3-none-any.whl.metadata (2.9 kB)\n",
      "  Using cached langchain_community-0.3.8-py3-none-any.whl.metadata (2.9 kB)\n",
      "Collecting SQLAlchemy<3,>=1.4 (from langchain==0.2.0)\n",
      "  Using cached SQLAlchemy-2.0.35-cp312-cp312-win_amd64.whl.metadata (9.9 kB)\n",
      "Collecting langchain-community\n",
      "  Using cached langchain_community-0.3.7-py3-none-any.whl.metadata (2.9 kB)\n",
      "INFO: pip is still looking at multiple versions of langchain-community to determine which version is compatible with other requirements. This could take a while.\n",
      "  Using cached langchain_community-0.3.6-py3-none-any.whl.metadata (2.9 kB)\n",
      "  Using cached langchain_community-0.3.5-py3-none-any.whl.metadata (2.9 kB)\n",
      "  Using cached langchain_community-0.3.4-py3-none-any.whl.metadata (2.9 kB)\n",
      "  Using cached langchain_community-0.3.3-py3-none-any.whl.metadata (2.8 kB)\n",
      "  Using cached langchain_community-0.3.2-py3-none-any.whl.metadata (2.8 kB)\n",
      "INFO: This is taking longer than usual. You might need to provide the dependency resolver with stricter constraints to reduce runtime. See https://pip.pypa.io/warnings/backtracking for guidance. If you want to abort this run, press Ctrl + C.\n",
      "  Using cached langchain_community-0.3.1-py3-none-any.whl.metadata (2.8 kB)\n",
      "  Using cached langchain_community-0.3.0-py3-none-any.whl.metadata (2.8 kB)\n",
      "  Using cached langchain_community-0.2.19-py3-none-any.whl.metadata (2.7 kB)\n",
      "  Using cached langchain_community-0.2.18-py3-none-any.whl.metadata (2.7 kB)\n",
      "  Using cached langchain_community-0.2.17-py3-none-any.whl.metadata (2.7 kB)\n",
      "  Using cached langchain_community-0.2.16-py3-none-any.whl.metadata (2.7 kB)\n",
      "  Using cached langchain_community-0.2.15-py3-none-any.whl.metadata (2.7 kB)\n",
      "  Using cached langchain_community-0.2.13-py3-none-any.whl.metadata (2.7 kB)\n",
      "  Using cached langchain_community-0.2.12-py3-none-any.whl.metadata (2.7 kB)\n",
      "  Using cached langchain_community-0.2.11-py3-none-any.whl.metadata (2.7 kB)\n",
      "  Using cached langchain_community-0.2.10-py3-none-any.whl.metadata (2.7 kB)\n",
      "  Using cached langchain_community-0.2.9-py3-none-any.whl.metadata (2.5 kB)\n",
      "  Using cached langchain_community-0.2.7-py3-none-any.whl.metadata (2.5 kB)\n",
      "  Using cached langchain_community-0.2.6-py3-none-any.whl.metadata (2.5 kB)\n",
      "  Using cached langchain_community-0.2.5-py3-none-any.whl.metadata (2.5 kB)\n",
      "  Using cached langchain_community-0.2.4-py3-none-any.whl.metadata (2.4 kB)\n",
      "Collecting transformers<5.0.0,>=4.41.0 (from sentence-transformers)\n",
      "  Using cached transformers-4.47.1-py3-none-any.whl.metadata (44 kB)\n",
      "Collecting torch>=1.11.0 (from sentence-transformers)\n",
      "  Using cached torch-2.5.1-cp312-cp312-win_amd64.whl.metadata (28 kB)\n",
      "Collecting scikit-learn (from sentence-transformers)\n",
      "  Using cached scikit_learn-1.6.0-cp312-cp312-win_amd64.whl.metadata (15 kB)\n",
      "Collecting scipy (from sentence-transformers)\n",
      "  Downloading scipy-1.15.0-cp312-cp312-win_amd64.whl.metadata (60 kB)\n",
      "Collecting huggingface-hub>=0.20.0 (from sentence-transformers)\n",
      "  Downloading huggingface_hub-0.27.1-py3-none-any.whl.metadata (13 kB)\n",
      "Collecting Pillow (from sentence-transformers)\n",
      "  Downloading pillow-11.1.0-cp312-cp312-win_amd64.whl.metadata (9.3 kB)\n",
      "Collecting aiohappyeyeballs>=2.3.0 (from aiohttp<4.0.0,>=3.8.3->langchain==0.2.0)\n",
      "  Using cached aiohappyeyeballs-2.4.4-py3-none-any.whl.metadata (6.1 kB)\n",
      "Collecting aiosignal>=1.1.2 (from aiohttp<4.0.0,>=3.8.3->langchain==0.2.0)\n",
      "  Using cached aiosignal-1.3.2-py2.py3-none-any.whl.metadata (3.8 kB)\n",
      "Requirement already satisfied: attrs>=17.3.0 in c:\\users\\bhowm\\.conda\\envs\\new1\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain==0.2.0) (24.3.0)\n",
      "Collecting frozenlist>=1.1.1 (from aiohttp<4.0.0,>=3.8.3->langchain==0.2.0)\n",
      "  Using cached frozenlist-1.5.0-cp312-cp312-win_amd64.whl.metadata (14 kB)\n",
      "Collecting multidict<7.0,>=4.5 (from aiohttp<4.0.0,>=3.8.3->langchain==0.2.0)\n",
      "  Using cached multidict-6.1.0-cp312-cp312-win_amd64.whl.metadata (5.1 kB)\n",
      "Collecting propcache>=0.2.0 (from aiohttp<4.0.0,>=3.8.3->langchain==0.2.0)\n",
      "  Using cached propcache-0.2.1-cp312-cp312-win_amd64.whl.metadata (9.5 kB)\n",
      "Collecting yarl<2.0,>=1.17.0 (from aiohttp<4.0.0,>=3.8.3->langchain==0.2.0)\n",
      "  Using cached yarl-1.18.3-cp312-cp312-win_amd64.whl.metadata (71 kB)\n",
      "Collecting marshmallow<4.0.0,>=3.18.0 (from dataclasses-json<0.7,>=0.5.7->langchain==0.2.0)\n",
      "  Downloading marshmallow-3.24.1-py3-none-any.whl.metadata (7.1 kB)\n",
      "Collecting typing-inspect<1,>=0.4.0 (from dataclasses-json<0.7,>=0.5.7->langchain==0.2.0)\n",
      "  Using cached typing_inspect-0.9.0-py3-none-any.whl.metadata (1.5 kB)\n",
      "Requirement already satisfied: starlette<0.28.0,>=0.27.0 in c:\\users\\bhowm\\appdata\\roaming\\python\\python312\\site-packages (from fastapi<0.100.0,>=0.95.2->chromadb==0.4.0) (0.27.0)\n",
      "Collecting filelock (from huggingface-hub>=0.20.0->sentence-transformers)\n",
      "  Using cached filelock-3.16.1-py3-none-any.whl.metadata (2.9 kB)\n",
      "Collecting fsspec>=2023.5.0 (from huggingface-hub>=0.20.0->sentence-transformers)\n",
      "  Using cached fsspec-2024.12.0-py3-none-any.whl.metadata (11 kB)\n",
      "Requirement already satisfied: packaging>=20.9 in c:\\users\\bhowm\\.conda\\envs\\new1\\lib\\site-packages (from huggingface-hub>=0.20.0->sentence-transformers) (24.2)\n",
      "Collecting jsonpatch<2.0,>=1.33 (from langchain-core<0.3.0,>=0.2.0->langchain==0.2.0)\n",
      "  Using cached jsonpatch-1.33-py2.py3-none-any.whl.metadata (3.0 kB)\n",
      "INFO: pip is looking at multiple versions of langchain-core to determine which version is compatible with other requirements. This could take a while.\n",
      "Collecting langchain-core<0.3.0,>=0.2.0 (from langchain==0.2.0)\n",
      "  Using cached langchain_core-0.2.42-py3-none-any.whl.metadata (6.2 kB)\n",
      "  Using cached langchain_core-0.2.41-py3-none-any.whl.metadata (6.2 kB)\n",
      "  Using cached langchain_core-0.2.40-py3-none-any.whl.metadata (6.2 kB)\n",
      "  Using cached langchain_core-0.2.39-py3-none-any.whl.metadata (6.2 kB)\n",
      "  Using cached langchain_core-0.2.38-py3-none-any.whl.metadata (6.2 kB)\n",
      "  Using cached langchain_core-0.2.37-py3-none-any.whl.metadata (6.2 kB)\n",
      "  Using cached langchain_core-0.2.36-py3-none-any.whl.metadata (6.2 kB)\n",
      "INFO: pip is still looking at multiple versions of langchain-core to determine which version is compatible with other requirements. This could take a while.\n",
      "  Using cached langchain_core-0.2.35-py3-none-any.whl.metadata (6.2 kB)\n",
      "  Using cached langchain_core-0.2.34-py3-none-any.whl.metadata (6.2 kB)\n",
      "  Using cached langchain_core-0.2.33-py3-none-any.whl.metadata (6.2 kB)\n",
      "  Using cached langchain_core-0.2.32-py3-none-any.whl.metadata (6.2 kB)\n",
      "  Using cached langchain_core-0.2.30-py3-none-any.whl.metadata (6.2 kB)\n",
      "INFO: This is taking longer than usual. You might need to provide the dependency resolver with stricter constraints to reduce runtime. See https://pip.pypa.io/warnings/backtracking for guidance. If you want to abort this run, press Ctrl + C.\n",
      "  Using cached langchain_core-0.2.29-py3-none-any.whl.metadata (6.2 kB)\n",
      "  Using cached langchain_core-0.2.28-py3-none-any.whl.metadata (6.2 kB)\n",
      "  Using cached langchain_core-0.2.27-py3-none-any.whl.metadata (6.2 kB)\n",
      "  Using cached langchain_core-0.2.26-py3-none-any.whl.metadata (6.2 kB)\n",
      "  Using cached langchain_core-0.2.25-py3-none-any.whl.metadata (6.2 kB)\n",
      "  Using cached langchain_core-0.2.24-py3-none-any.whl.metadata (6.2 kB)\n",
      "  Using cached langchain_core-0.2.23-py3-none-any.whl.metadata (6.2 kB)\n",
      "  Using cached langchain_core-0.2.22-py3-none-any.whl.metadata (6.0 kB)\n",
      "  Using cached langchain_core-0.2.21-py3-none-any.whl.metadata (6.0 kB)\n",
      "  Using cached langchain_core-0.2.20-py3-none-any.whl.metadata (6.0 kB)\n",
      "  Using cached langchain_core-0.2.19-py3-none-any.whl.metadata (6.0 kB)\n",
      "  Using cached langchain_core-0.2.18-py3-none-any.whl.metadata (6.0 kB)\n",
      "  Using cached langchain_core-0.2.17-py3-none-any.whl.metadata (6.0 kB)\n",
      "  Using cached langchain_core-0.2.15-py3-none-any.whl.metadata (6.0 kB)\n",
      "  Using cached langchain_core-0.2.13-py3-none-any.whl.metadata (6.0 kB)\n",
      "  Using cached langchain_core-0.2.12-py3-none-any.whl.metadata (6.0 kB)\n",
      "  Using cached langchain_core-0.2.11-py3-none-any.whl.metadata (6.0 kB)\n",
      "  Using cached langchain_core-0.2.10-py3-none-any.whl.metadata (6.0 kB)\n",
      "  Using cached langchain_core-0.2.9-py3-none-any.whl.metadata (6.0 kB)\n",
      "  Using cached langchain_core-0.2.8-py3-none-any.whl.metadata (5.8 kB)\n",
      "Collecting langchain-text-splitters<0.3.0,>=0.2.0 (from langchain==0.2.0)\n",
      "  Using cached langchain_text_splitters-0.2.2-py3-none-any.whl.metadata (2.1 kB)\n",
      "  Using cached langchain_text_splitters-0.2.1-py3-none-any.whl.metadata (2.2 kB)\n",
      "Requirement already satisfied: httpx<1,>=0.23.0 in c:\\users\\bhowm\\.conda\\envs\\new1\\lib\\site-packages (from langsmith<0.2.0,>=0.1.17->langchain==0.2.0) (0.27.0)\n",
      "Collecting orjson<4.0.0,>=3.9.14 (from langsmith<0.2.0,>=0.1.17->langchain==0.2.0)\n",
      "  Downloading orjson-3.10.13-cp312-cp312-win_amd64.whl.metadata (42 kB)\n",
      "INFO: pip is looking at multiple versions of langsmith to determine which version is compatible with other requirements. This could take a while.\n",
      "Collecting langsmith<0.2.0,>=0.1.17 (from langchain==0.2.0)\n",
      "  Using cached langsmith-0.1.146-py3-none-any.whl.metadata (14 kB)\n",
      "  Using cached langsmith-0.1.145-py3-none-any.whl.metadata (14 kB)\n",
      "  Using cached langsmith-0.1.144-py3-none-any.whl.metadata (14 kB)\n",
      "  Using cached langsmith-0.1.143-py3-none-any.whl.metadata (13 kB)\n",
      "  Using cached langsmith-0.1.142-py3-none-any.whl.metadata (13 kB)\n",
      "  Using cached langsmith-0.1.139-py3-none-any.whl.metadata (13 kB)\n",
      "  Using cached langsmith-0.1.138-py3-none-any.whl.metadata (13 kB)\n",
      "INFO: pip is still looking at multiple versions of langsmith to determine which version is compatible with other requirements. This could take a while.\n",
      "  Using cached langsmith-0.1.137-py3-none-any.whl.metadata (13 kB)\n",
      "  Using cached langsmith-0.1.134-py3-none-any.whl.metadata (13 kB)\n",
      "  Using cached langsmith-0.1.133-py3-none-any.whl.metadata (13 kB)\n",
      "  Using cached langsmith-0.1.132-py3-none-any.whl.metadata (13 kB)\n",
      "  Using cached langsmith-0.1.131-py3-none-any.whl.metadata (13 kB)\n",
      "INFO: This is taking longer than usual. You might need to provide the dependency resolver with stricter constraints to reduce runtime. See https://pip.pypa.io/warnings/backtracking for guidance. If you want to abort this run, press Ctrl + C.\n",
      "  Using cached langsmith-0.1.129-py3-none-any.whl.metadata (13 kB)\n",
      "  Using cached langsmith-0.1.128-py3-none-any.whl.metadata (13 kB)\n",
      "  Using cached langsmith-0.1.127-py3-none-any.whl.metadata (13 kB)\n",
      "  Using cached langsmith-0.1.126-py3-none-any.whl.metadata (13 kB)\n",
      "  Using cached langsmith-0.1.125-py3-none-any.whl.metadata (13 kB)\n",
      "  Using cached langsmith-0.1.124-py3-none-any.whl.metadata (13 kB)\n",
      "  Using cached langsmith-0.1.123-py3-none-any.whl.metadata (13 kB)\n",
      "  Using cached langsmith-0.1.122-py3-none-any.whl.metadata (13 kB)\n",
      "  Using cached langsmith-0.1.121-py3-none-any.whl.metadata (13 kB)\n",
      "  Using cached langsmith-0.1.120-py3-none-any.whl.metadata (13 kB)\n",
      "  Using cached langsmith-0.1.119-py3-none-any.whl.metadata (13 kB)\n",
      "  Using cached langsmith-0.1.118-py3-none-any.whl.metadata (13 kB)\n",
      "  Using cached langsmith-0.1.117-py3-none-any.whl.metadata (13 kB)\n",
      "  Using cached langsmith-0.1.116-py3-none-any.whl.metadata (13 kB)\n",
      "  Using cached langsmith-0.1.115-py3-none-any.whl.metadata (13 kB)\n",
      "  Using cached langsmith-0.1.114-py3-none-any.whl.metadata (13 kB)\n",
      "  Using cached langsmith-0.1.113-py3-none-any.whl.metadata (13 kB)\n",
      "  Using cached langsmith-0.1.112-py3-none-any.whl.metadata (13 kB)\n",
      "  Using cached langsmith-0.1.111-py3-none-any.whl.metadata (13 kB)\n",
      "  Using cached langsmith-0.1.110-py3-none-any.whl.metadata (13 kB)\n",
      "  Using cached langsmith-0.1.109-py3-none-any.whl.metadata (13 kB)\n",
      "  Using cached langsmith-0.1.108-py3-none-any.whl.metadata (13 kB)\n",
      "  Using cached langsmith-0.1.107-py3-none-any.whl.metadata (13 kB)\n",
      "  Using cached langsmith-0.1.106-py3-none-any.whl.metadata (13 kB)\n",
      "  Using cached langsmith-0.1.105-py3-none-any.whl.metadata (13 kB)\n",
      "  Using cached langsmith-0.1.104-py3-none-any.whl.metadata (13 kB)\n",
      "  Using cached langsmith-0.1.103-py3-none-any.whl.metadata (13 kB)\n",
      "  Using cached langsmith-0.1.102-py3-none-any.whl.metadata (13 kB)\n",
      "  Using cached langsmith-0.1.101-py3-none-any.whl.metadata (13 kB)\n",
      "  Using cached langsmith-0.1.99-py3-none-any.whl.metadata (13 kB)\n",
      "  Using cached langsmith-0.1.98-py3-none-any.whl.metadata (13 kB)\n",
      "  Using cached langsmith-0.1.97-py3-none-any.whl.metadata (13 kB)\n",
      "  Using cached langsmith-0.1.96-py3-none-any.whl.metadata (13 kB)\n",
      "  Using cached langsmith-0.1.95-py3-none-any.whl.metadata (13 kB)\n",
      "  Using cached langsmith-0.1.94-py3-none-any.whl.metadata (13 kB)\n",
      "  Using cached langsmith-0.1.93-py3-none-any.whl.metadata (13 kB)\n",
      "  Using cached langsmith-0.1.92-py3-none-any.whl.metadata (13 kB)\n",
      "  Using cached langsmith-0.1.91-py3-none-any.whl.metadata (13 kB)\n",
      "  Using cached langsmith-0.1.90-py3-none-any.whl.metadata (13 kB)\n",
      "  Using cached langsmith-0.1.89-py3-none-any.whl.metadata (13 kB)\n",
      "  Using cached langsmith-0.1.88-py3-none-any.whl.metadata (13 kB)\n",
      "  Using cached langsmith-0.1.87-py3-none-any.whl.metadata (13 kB)\n",
      "  Using cached langsmith-0.1.86-py3-none-any.whl.metadata (13 kB)\n",
      "  Using cached langsmith-0.1.85-py3-none-any.whl.metadata (13 kB)\n",
      "  Using cached langsmith-0.1.84-py3-none-any.whl.metadata (13 kB)\n",
      "  Using cached langsmith-0.1.83-py3-none-any.whl.metadata (13 kB)\n",
      "  Using cached langsmith-0.1.82-py3-none-any.whl.metadata (13 kB)\n",
      "  Using cached langsmith-0.1.81-py3-none-any.whl.metadata (13 kB)\n",
      "Requirement already satisfied: coloredlogs in c:\\users\\bhowm\\appdata\\roaming\\python\\python312\\site-packages (from onnxruntime>=1.14.1->chromadb==0.4.0) (15.0.1)\n",
      "Requirement already satisfied: flatbuffers in c:\\users\\bhowm\\appdata\\roaming\\python\\python312\\site-packages (from onnxruntime>=1.14.1->chromadb==0.4.0) (24.3.25)\n",
      "Requirement already satisfied: protobuf in c:\\users\\bhowm\\appdata\\roaming\\python\\python312\\site-packages (from onnxruntime>=1.14.1->chromadb==0.4.0) (5.29.2)\n",
      "Collecting sympy (from onnxruntime>=1.14.1->chromadb==0.4.0)\n",
      "  Using cached sympy-1.13.3-py3-none-any.whl.metadata (12 kB)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\bhowm\\.conda\\envs\\new1\\lib\\site-packages (from pandas>=1.3->chromadb==0.4.0) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\bhowm\\.conda\\envs\\new1\\lib\\site-packages (from pandas>=1.3->chromadb==0.4.0) (2024.1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in c:\\users\\bhowm\\appdata\\roaming\\python\\python312\\site-packages (from pandas>=1.3->chromadb==0.4.0) (2024.2)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\bhowm\\.conda\\envs\\new1\\lib\\site-packages (from posthog>=2.4.0->chromadb==0.4.0) (1.16.0)\n",
      "Requirement already satisfied: monotonic>=1.5 in c:\\users\\bhowm\\appdata\\roaming\\python\\python312\\site-packages (from posthog>=2.4.0->chromadb==0.4.0) (1.6)\n",
      "Requirement already satisfied: backoff>=1.10.0 in c:\\users\\bhowm\\appdata\\roaming\\python\\python312\\site-packages (from posthog>=2.4.0->chromadb==0.4.0) (2.2.1)\n",
      "Requirement already satisfied: certifi in c:\\users\\bhowm\\.conda\\envs\\new1\\lib\\site-packages (from pulsar-client>=3.1.0->chromadb==0.4.0) (2024.12.14)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\bhowm\\.conda\\envs\\new1\\lib\\site-packages (from requests>=2.28->chromadb==0.4.0) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\bhowm\\.conda\\envs\\new1\\lib\\site-packages (from requests>=2.28->chromadb==0.4.0) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\bhowm\\.conda\\envs\\new1\\lib\\site-packages (from requests>=2.28->chromadb==0.4.0) (2.2.3)\n",
      "Collecting greenlet!=0.4.17 (from SQLAlchemy<3,>=1.4->langchain==0.2.0)\n",
      "  Using cached greenlet-3.1.1-cp312-cp312-win_amd64.whl.metadata (3.9 kB)\n",
      "Collecting networkx (from torch>=1.11.0->sentence-transformers)\n",
      "  Using cached networkx-3.4.2-py3-none-any.whl.metadata (6.3 kB)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\bhowm\\.conda\\envs\\new1\\lib\\site-packages (from torch>=1.11.0->sentence-transformers) (3.1.4)\n",
      "Requirement already satisfied: setuptools in c:\\users\\bhowm\\.conda\\envs\\new1\\lib\\site-packages (from torch>=1.11.0->sentence-transformers) (75.1.0)\n",
      "Collecting sympy (from onnxruntime>=1.14.1->chromadb==0.4.0)\n",
      "  Using cached sympy-1.13.1-py3-none-any.whl.metadata (12 kB)\n",
      "Collecting mpmath<1.4,>=1.1.0 (from sympy->onnxruntime>=1.14.1->chromadb==0.4.0)\n",
      "  Using cached mpmath-1.3.0-py3-none-any.whl.metadata (8.6 kB)\n",
      "Requirement already satisfied: colorama in c:\\users\\bhowm\\.conda\\envs\\new1\\lib\\site-packages (from tqdm>=4.65.0->chromadb==0.4.0) (0.4.6)\n",
      "Collecting regex!=2019.12.17 (from transformers<5.0.0,>=4.41.0->sentence-transformers)\n",
      "  Using cached regex-2024.11.6-cp312-cp312-win_amd64.whl.metadata (41 kB)\n",
      "Collecting safetensors>=0.4.1 (from transformers<5.0.0,>=4.41.0->sentence-transformers)\n",
      "  Downloading safetensors-0.5.0-cp38-abi3-win_amd64.whl.metadata (3.9 kB)\n",
      "Requirement already satisfied: click>=7.0 in c:\\users\\bhowm\\appdata\\roaming\\python\\python312\\site-packages (from uvicorn>=0.18.3->uvicorn[standard]>=0.18.3->chromadb==0.4.0) (8.1.7)\n",
      "Requirement already satisfied: h11>=0.8 in c:\\users\\bhowm\\.conda\\envs\\new1\\lib\\site-packages (from uvicorn>=0.18.3->uvicorn[standard]>=0.18.3->chromadb==0.4.0) (0.14.0)\n",
      "Requirement already satisfied: httptools>=0.6.3 in c:\\users\\bhowm\\appdata\\roaming\\python\\python312\\site-packages (from uvicorn[standard]>=0.18.3->chromadb==0.4.0) (0.6.4)\n",
      "Collecting python-dotenv>=0.13 (from uvicorn[standard]>=0.18.3->chromadb==0.4.0)\n",
      "  Using cached python_dotenv-1.0.1-py3-none-any.whl.metadata (23 kB)\n",
      "Requirement already satisfied: watchfiles>=0.13 in c:\\users\\bhowm\\appdata\\roaming\\python\\python312\\site-packages (from uvicorn[standard]>=0.18.3->chromadb==0.4.0) (1.0.3)\n",
      "Requirement already satisfied: websockets>=10.4 in c:\\users\\bhowm\\appdata\\roaming\\python\\python312\\site-packages (from uvicorn[standard]>=0.18.3->chromadb==0.4.0) (14.1)\n",
      "Collecting joblib>=1.2.0 (from scikit-learn->sentence-transformers)\n",
      "  Using cached joblib-1.4.2-py3-none-any.whl.metadata (5.4 kB)\n",
      "Collecting threadpoolctl>=3.1.0 (from scikit-learn->sentence-transformers)\n",
      "  Using cached threadpoolctl-3.5.0-py3-none-any.whl.metadata (13 kB)\n",
      "Collecting jsonpointer>=1.9 (from jsonpatch<2.0,>=1.33->langchain-core<0.3.0,>=0.2.0->langchain==0.2.0)\n",
      "  Using cached jsonpointer-3.0.0-py2.py3-none-any.whl.metadata (2.3 kB)\n",
      "Requirement already satisfied: anyio<5,>=3.4.0 in c:\\users\\bhowm\\.conda\\envs\\new1\\lib\\site-packages (from starlette<0.28.0,>=0.27.0->fastapi<0.100.0,>=0.95.2->chromadb==0.4.0) (4.6.2)\n",
      "Collecting mypy-extensions>=0.3.0 (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7,>=0.5.7->langchain==0.2.0)\n",
      "  Using cached mypy_extensions-1.0.0-py3-none-any.whl.metadata (1.1 kB)\n",
      "Requirement already satisfied: humanfriendly>=9.1 in c:\\users\\bhowm\\appdata\\roaming\\python\\python312\\site-packages (from coloredlogs->onnxruntime>=1.14.1->chromadb==0.4.0) (10.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\bhowm\\.conda\\envs\\new1\\lib\\site-packages (from jinja2->torch>=1.11.0->sentence-transformers) (2.1.3)\n",
      "Requirement already satisfied: sniffio>=1.1 in c:\\users\\bhowm\\.conda\\envs\\new1\\lib\\site-packages (from anyio<5,>=3.4.0->starlette<0.28.0,>=0.27.0->fastapi<0.100.0,>=0.95.2->chromadb==0.4.0) (1.3.0)\n",
      "Requirement already satisfied: pyreadline3 in c:\\users\\bhowm\\appdata\\roaming\\python\\python312\\site-packages (from humanfriendly>=9.1->coloredlogs->onnxruntime>=1.14.1->chromadb==0.4.0) (3.5.4)\n",
      "Using cached langchain-0.2.0-py3-none-any.whl (973 kB)\n",
      "Using cached langchain_community-0.2.4-py3-none-any.whl (2.2 MB)\n",
      "Using cached sentence_transformers-3.3.1-py3-none-any.whl (268 kB)\n",
      "Using cached aiohttp-3.11.11-cp312-cp312-win_amd64.whl (437 kB)\n",
      "Using cached dataclasses_json-0.6.7-py3-none-any.whl (28 kB)\n",
      "Downloading huggingface_hub-0.27.1-py3-none-any.whl (450 kB)\n",
      "Using cached langchain_core-0.2.8-py3-none-any.whl (315 kB)\n",
      "Using cached pydantic-1.10.19-cp312-cp312-win_amd64.whl (2.0 MB)\n",
      "Using cached langchain_text_splitters-0.2.1-py3-none-any.whl (23 kB)\n",
      "Using cached langsmith-0.1.81-py3-none-any.whl (127 kB)\n",
      "Using cached numpy-1.26.4-cp312-cp312-win_amd64.whl (15.5 MB)\n",
      "Using cached SQLAlchemy-2.0.36-cp312-cp312-win_amd64.whl (2.1 MB)\n",
      "Using cached tenacity-8.5.0-py3-none-any.whl (28 kB)\n",
      "Using cached tokenizers-0.21.0-cp39-abi3-win_amd64.whl (2.4 MB)\n",
      "Using cached torch-2.5.1-cp312-cp312-win_amd64.whl (203.0 MB)\n",
      "Using cached sympy-1.13.1-py3-none-any.whl (6.2 MB)\n",
      "Using cached tqdm-4.67.1-py3-none-any.whl (78 kB)\n",
      "Using cached transformers-4.47.1-py3-none-any.whl (10.1 MB)\n",
      "Downloading pillow-11.1.0-cp312-cp312-win_amd64.whl (2.6 MB)\n",
      "   ---------------------------------------- 0.0/2.6 MB ? eta -:--:--\n",
      "   ------------------------------- -------- 2.1/2.6 MB 9.8 MB/s eta 0:00:01\n",
      "   ---------------------------------------  2.6/2.6 MB 8.4 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 2.6/2.6 MB 4.0 MB/s eta 0:00:00\n",
      "Using cached scikit_learn-1.6.0-cp312-cp312-win_amd64.whl (11.1 MB)\n",
      "Downloading scipy-1.15.0-cp312-cp312-win_amd64.whl (43.6 MB)\n",
      "   ---------------------------------------- 0.0/43.6 MB ? eta -:--:--\n",
      "   - -------------------------------------- 1.6/43.6 MB 7.0 MB/s eta 0:00:06\n",
      "   -- ------------------------------------- 2.6/43.6 MB 6.6 MB/s eta 0:00:07\n",
      "   -- ------------------------------------- 2.6/43.6 MB 6.6 MB/s eta 0:00:07\n",
      "   -- ------------------------------------- 2.6/43.6 MB 6.6 MB/s eta 0:00:07\n",
      "   -- ------------------------------------- 2.6/43.6 MB 6.6 MB/s eta 0:00:07\n",
      "   -- ------------------------------------- 3.1/43.6 MB 2.5 MB/s eta 0:00:17\n",
      "   --- ------------------------------------ 3.7/43.6 MB 2.5 MB/s eta 0:00:16\n",
      "   --- ------------------------------------ 3.9/43.6 MB 2.4 MB/s eta 0:00:17\n",
      "   --- ------------------------------------ 3.9/43.6 MB 2.4 MB/s eta 0:00:17\n",
      "   ---- ----------------------------------- 4.5/43.6 MB 2.1 MB/s eta 0:00:19\n",
      "   ---- ----------------------------------- 4.7/43.6 MB 2.1 MB/s eta 0:00:19\n",
      "   ---- ----------------------------------- 5.0/43.6 MB 1.9 MB/s eta 0:00:20\n",
      "   ---- ----------------------------------- 5.0/43.6 MB 1.9 MB/s eta 0:00:20\n",
      "   ---- ----------------------------------- 5.2/43.6 MB 1.7 MB/s eta 0:00:23\n",
      "   ---- ----------------------------------- 5.2/43.6 MB 1.7 MB/s eta 0:00:23\n",
      "   ----- ---------------------------------- 5.5/43.6 MB 1.6 MB/s eta 0:00:24\n",
      "   ----- ---------------------------------- 5.8/43.6 MB 1.6 MB/s eta 0:00:25\n",
      "   ----- ---------------------------------- 5.8/43.6 MB 1.6 MB/s eta 0:00:25\n",
      "   ----- ---------------------------------- 5.8/43.6 MB 1.6 MB/s eta 0:00:25\n",
      "   ----- ---------------------------------- 6.0/43.6 MB 1.4 MB/s eta 0:00:27\n",
      "   ----- ---------------------------------- 6.0/43.6 MB 1.4 MB/s eta 0:00:27\n",
      "   ----- ---------------------------------- 6.0/43.6 MB 1.4 MB/s eta 0:00:27\n",
      "   ----- ---------------------------------- 6.0/43.6 MB 1.4 MB/s eta 0:00:27\n",
      "   ----- ---------------------------------- 6.3/43.6 MB 1.2 MB/s eta 0:00:31\n",
      "   ------ --------------------------------- 6.6/43.6 MB 1.3 MB/s eta 0:00:30\n",
      "   ------ --------------------------------- 7.3/43.6 MB 1.3 MB/s eta 0:00:28\n",
      "   ------- -------------------------------- 8.4/43.6 MB 1.5 MB/s eta 0:00:25\n",
      "   -------- ------------------------------- 9.4/43.6 MB 1.6 MB/s eta 0:00:22\n",
      "   --------- ------------------------------ 10.5/43.6 MB 1.7 MB/s eta 0:00:20\n",
      "   ---------- ----------------------------- 11.5/43.6 MB 1.8 MB/s eta 0:00:18\n",
      "   ----------- ---------------------------- 12.6/43.6 MB 1.9 MB/s eta 0:00:17\n",
      "   ------------ --------------------------- 13.4/43.6 MB 2.0 MB/s eta 0:00:16\n",
      "   ------------- -------------------------- 14.7/43.6 MB 2.1 MB/s eta 0:00:14\n",
      "   -------------- ------------------------- 15.7/43.6 MB 2.2 MB/s eta 0:00:13\n",
      "   --------------- ------------------------ 16.5/43.6 MB 2.3 MB/s eta 0:00:13\n",
      "   ---------------- ----------------------- 17.6/43.6 MB 2.3 MB/s eta 0:00:12\n",
      "   ----------------- ---------------------- 18.9/43.6 MB 2.4 MB/s eta 0:00:11\n",
      "   ----------------- ---------------------- 19.4/43.6 MB 2.4 MB/s eta 0:00:10\n",
      "   ------------------ --------------------- 20.4/43.6 MB 2.5 MB/s eta 0:00:10\n",
      "   ------------------- -------------------- 21.8/43.6 MB 2.6 MB/s eta 0:00:09\n",
      "   -------------------- ------------------- 22.5/43.6 MB 2.6 MB/s eta 0:00:09\n",
      "   --------------------- ------------------ 23.6/43.6 MB 2.7 MB/s eta 0:00:08\n",
      "   ---------------------- ----------------- 24.6/43.6 MB 2.7 MB/s eta 0:00:07\n",
      "   ----------------------- ---------------- 26.0/43.6 MB 2.8 MB/s eta 0:00:07\n",
      "   ------------------------ --------------- 26.7/43.6 MB 2.8 MB/s eta 0:00:06\n",
      "   ------------------------- -------------- 27.5/43.6 MB 2.9 MB/s eta 0:00:06\n",
      "   ------------------------- -------------- 28.3/43.6 MB 2.9 MB/s eta 0:00:06\n",
      "   --------------------------- ------------ 29.6/43.6 MB 3.0 MB/s eta 0:00:05\n",
      "   --------------------------- ------------ 30.4/43.6 MB 3.0 MB/s eta 0:00:05\n",
      "   ---------------------------- ----------- 30.9/43.6 MB 3.0 MB/s eta 0:00:05\n",
      "   ----------------------------- ---------- 32.2/43.6 MB 3.0 MB/s eta 0:00:04\n",
      "   ------------------------------ --------- 33.3/43.6 MB 3.1 MB/s eta 0:00:04\n",
      "   ------------------------------- -------- 34.6/43.6 MB 3.1 MB/s eta 0:00:03\n",
      "   -------------------------------- ------- 35.7/43.6 MB 3.2 MB/s eta 0:00:03\n",
      "   --------------------------------- ------ 36.4/43.6 MB 3.2 MB/s eta 0:00:03\n",
      "   ---------------------------------- ----- 37.2/43.6 MB 3.2 MB/s eta 0:00:03\n",
      "   ----------------------------------- ---- 38.5/43.6 MB 3.2 MB/s eta 0:00:02\n",
      "   ------------------------------------ --- 39.8/43.6 MB 3.3 MB/s eta 0:00:02\n",
      "   ------------------------------------- -- 40.4/43.6 MB 3.3 MB/s eta 0:00:01\n",
      "   ------------------------------------- -- 41.4/43.6 MB 3.3 MB/s eta 0:00:01\n",
      "   -------------------------------------- - 42.5/43.6 MB 3.4 MB/s eta 0:00:01\n",
      "   ---------------------------------------  43.5/43.6 MB 3.4 MB/s eta 0:00:01\n",
      "   ---------------------------------------  43.5/43.6 MB 3.4 MB/s eta 0:00:01\n",
      "   ---------------------------------------  43.5/43.6 MB 3.4 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 43.6/43.6 MB 3.2 MB/s eta 0:00:00\n",
      "Using cached aiohappyeyeballs-2.4.4-py3-none-any.whl (14 kB)\n",
      "Using cached aiosignal-1.3.2-py2.py3-none-any.whl (7.6 kB)\n",
      "Using cached frozenlist-1.5.0-cp312-cp312-win_amd64.whl (51 kB)\n",
      "Using cached fsspec-2024.12.0-py3-none-any.whl (183 kB)\n",
      "Using cached greenlet-3.1.1-cp312-cp312-win_amd64.whl (299 kB)\n",
      "Using cached joblib-1.4.2-py3-none-any.whl (301 kB)\n",
      "Using cached jsonpatch-1.33-py2.py3-none-any.whl (12 kB)\n",
      "Downloading marshmallow-3.24.1-py3-none-any.whl (49 kB)\n",
      "Using cached multidict-6.1.0-cp312-cp312-win_amd64.whl (28 kB)\n",
      "Downloading orjson-3.10.13-cp312-cp312-win_amd64.whl (135 kB)\n",
      "Using cached propcache-0.2.1-cp312-cp312-win_amd64.whl (44 kB)\n",
      "Using cached python_dotenv-1.0.1-py3-none-any.whl (19 kB)\n",
      "Using cached regex-2024.11.6-cp312-cp312-win_amd64.whl (273 kB)\n",
      "Downloading safetensors-0.5.0-cp38-abi3-win_amd64.whl (303 kB)\n",
      "Using cached threadpoolctl-3.5.0-py3-none-any.whl (18 kB)\n",
      "Using cached typing_inspect-0.9.0-py3-none-any.whl (8.8 kB)\n",
      "Using cached yarl-1.18.3-cp312-cp312-win_amd64.whl (90 kB)\n",
      "Using cached filelock-3.16.1-py3-none-any.whl (16 kB)\n",
      "Using cached networkx-3.4.2-py3-none-any.whl (1.7 MB)\n",
      "Using cached jsonpointer-3.0.0-py2.py3-none-any.whl (7.6 kB)\n",
      "Using cached mpmath-1.3.0-py3-none-any.whl (536 kB)\n",
      "Using cached mypy_extensions-1.0.0-py3-none-any.whl (4.7 kB)\n",
      "Installing collected packages: mpmath, tqdm, threadpoolctl, tenacity, sympy, safetensors, regex, python-dotenv, pydantic, propcache, Pillow, orjson, numpy, networkx, mypy-extensions, multidict, marshmallow, jsonpointer, joblib, greenlet, fsspec, frozenlist, filelock, aiohappyeyeballs, yarl, typing-inspect, torch, SQLAlchemy, scipy, langsmith, jsonpatch, huggingface-hub, aiosignal, tokenizers, scikit-learn, langchain-core, dataclasses-json, aiohttp, transformers, langchain-text-splitters, sentence-transformers, langchain, langchain-community\n",
      "Successfully installed Pillow-11.1.0 SQLAlchemy-2.0.36 aiohappyeyeballs-2.4.4 aiohttp-3.11.11 aiosignal-1.3.2 dataclasses-json-0.6.7 filelock-3.16.1 frozenlist-1.5.0 fsspec-2024.12.0 greenlet-3.1.1 huggingface-hub-0.27.1 joblib-1.4.2 jsonpatch-1.33 jsonpointer-3.0.0 langchain-0.2.0 langchain-community-0.2.4 langchain-core-0.2.8 langchain-text-splitters-0.2.1 langsmith-0.1.81 marshmallow-3.24.1 mpmath-1.3.0 multidict-6.1.0 mypy-extensions-1.0.0 networkx-3.4.2 numpy-1.26.4 orjson-3.10.13 propcache-0.2.1 pydantic-1.10.19 python-dotenv-1.0.1 regex-2024.11.6 safetensors-0.5.0 scikit-learn-1.6.0 scipy-1.15.0 sentence-transformers-3.3.1 sympy-1.13.1 tenacity-8.5.0 threadpoolctl-3.5.0 tokenizers-0.21.0 torch-2.5.1 tqdm-4.67.1 transformers-4.47.1 typing-inspect-0.9.0 yarl-1.18.3\n"
     ]
    }
   ],
   "source": [
    "!pip install \"PyPDF2\" \"chromadb==0.4.0\" \"langchain==0.2.0\" \"langchain-community\" \"sentence-transformers\" "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d06946fa-0edf-4c84-a502-5805ee7da305",
   "metadata": {},
   "source": [
    "### Reading PDF reports and remove sensitive data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "eafb9c74-bc21-4c75-b78f-c0b7e5fa186f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from PyPDF2 import PdfReader\n",
    "def read_pdf(file_path):\n",
    "    # Initialize the reader for the PDF file\n",
    "    \n",
    "    reader = PdfReader(file_path)\n",
    "    \n",
    "    # Extract text from each page and store it in a variable\n",
    "    pdf_text = \"\"\n",
    "    for page in reader.pages:\n",
    "        pdf_text += page.extract_text() + \"\\n\"  # Adding a newline for separation between pages\n",
    "  \n",
    "    text = pdf_text.replace(\"\\n\", \" \")\n",
    "    \n",
    "    return text\n",
    "    #return page"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ee0b91ca-5ed2-4177-a2b1-94320a6c151d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "def remove_sensitive(text):\n",
    "    text = re.sub(r\"Patient\\s+Name\\s+:\\s+.*?\\s+(?=Patient\\s+ID)\", \"\", text)#removing patient name\n",
    "    text = re.sub(r\"Referring\\s+Physician\\s+.*?\\s+Report\", \"Report\", text)\n",
    "    text = re.sub(r\"Dr [A-Za-z\\s]+MD\\s*|Reg No:\\s*\\d+\", \"\", text).strip()\n",
    "    return text\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27fe0c51-a212-403e-b1c8-34f042a87249",
   "metadata": {},
   "source": [
    "### Creating field for json and removing other sensitive data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0683046c-7067-48a9-aed3-befa99815cbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import json\n",
    "def text_json(extracted_data):\n",
    "# Regex patterns to extract fields\n",
    "    patterns = {\n",
    "        \"Patient ID\": r\"Patient\\s+ID\\s*:\\s*(\\S+)\",\n",
    "        \"Date\": r\"Report\\s+Date\\s+/ Time\\s+:\\s+(\\d{1,2}\\s+\\w+\\s+\\d{4})\",\n",
    "        \"Time\": r\"(\\d{2}:\\d{2}:\\d{2})\",\n",
    "        \"Patient age\": r\"Patient\\s+age\\s+/\\s+Sex\\s+:\\s+(\\d{3}Y)\",\n",
    "        \"Sex\": r\"Sex\\s*:\\s*\\d{3}[A-Za-z]*\\s*/\\s*(\\w)\",\n",
    "        \"TECHNIQUE\": r\"TECHNIQUE\\s*:\\s*(.*?)\\s*FINDINGS\",\n",
    "        \"FINDINGS\": r\"FINDINGS\\s*:\\s*(.*?)\\s*IMPRESSION\",\n",
    "        \"IMPRESSION\": r\"IMPRESSION\\s*:\\s*(.*)\"\n",
    "    }\n",
    "    \n",
    "    # Extract fields using regex\n",
    "    data = {}\n",
    "    for key, pattern in patterns.items():\n",
    "        match = re.search(pattern, extracted_data, re.DOTALL)\n",
    "        if match:\n",
    "            data[key] = match.group(1).strip()\n",
    "    \n",
    "    # removes the unwanted portion from the \"FINDINGS\" and \"IMPRESSION\" field while keeping the rest of the content intact.\n",
    "    try:\n",
    "        findings = data[\"FINDINGS\"]\n",
    "        impression = data[\"IMPRESSION\"]\n",
    "        updated_findings = re.sub(r\"Patient\\s+ID\\s*:\\s*(\\S+).*Report\\s+Date\\s+/ Time\\s+:\\s+(\\d{1,2}\\s+\\w+\\s+\\d{4})\\s*(\\S+)\", \"\", findings).strip()\n",
    "        updated_impression = re.sub(r\"Patient\\s+ID\\s*:\\s*(\\S+).*Report\\s+Date\\s+/ Time\\s+:\\s+(\\d{1,2}\\s+\\w+\\s+\\d{4})\\s*(\\S+)\", \"\", impression).strip()\n",
    "\n",
    "        # Update the Data\n",
    "        data[\"FINDINGS\"] = updated_findings\n",
    "        data[\"IMPRESSION\"] = updated_impression\n",
    "    except:\n",
    "        print(\"no unwanted found\")\n",
    "    # Update the JSON\n",
    "    json_data = json.dumps(data, indent=4)\n",
    "    \n",
    "    return json_data\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6489beb2-af33-4b44-b029-37192d67202e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "    \"Patient ID\": \"MR0000444444\",\n",
      "    \"Date\": \"20 April  2020\",\n",
      "    \"Time\": \"10:16:04\",\n",
      "    \"Patient age\": \"075Y\",\n",
      "    \"Sex\": \"M\",\n",
      "    \"TECHNIQUE\": \"T2 FSE Axials  / Sagittals  & Coronals.   T1 &T2  FLAIR  Axials.  DWI,  GRE  Axials.    3D TOF  MR Angiography  of Intracranial  Arteries.\",\n",
      "    \"FINDINGS\": \"Large wedge shaped lesion with restricted diffusion low ADC values and hyperintensities  in T2 & FLAIR images noted involving capsuloganglionic region, parietal lobe, adjacent  frontal insular cortex, temporal lobe in right side.   Focal FLAIR hyperintensities without restricted diffusion in bilateral posterior  periventricular wh ite matter, left corona radiata and centrum semiovale.   Curvilinear  blooming  SWI hypointensities  in distal  right  MCA.   Cerebellum, 4th ventricle, brain stem & CP angle regions are within normal limits.  Sella, suprasellar & parasellar areas are normal.   The ex tracerebral spaces and supratentorial ventricular system are normal.  Rest of the cerebral parenchyma is normal is normal.   Midline structures and corpus callosum are normal.  No haemorrhagic pathology.   No extraaxial  collection  seen.   MR Angiography  shows :  Hypoplastic  left vertebral  artery.   Distal  left vertebral  artery  not well visualised.   Focal  mild stenosis  in basilar  artery  in P1 segment  of left PCA.        Non visualisation  of distal  2/3rds  of right  MCA  and distal  right  MCA  branches.   Bilateral intracranial internal carotid arteries, anterior & left middle cerebral arteries  and their branches are normal in caliber/flow signal.   Otherwise basilar artery, bilateral posterior cerebral, superior cerebellar and anterior  inferior cerebellar arteries are normal in ca liber and course.   Right vertebral artery is normal in calibre and flow signal.  No aneurysm / AVM seen.\",\n",
      "    \"IMPRESSION\": \"* Large acute non haemorrhagic infract in right MCA territory involving  capsuloganglionic region,  parietal lobe, adjacent frontal insular corte x,  temporal lobe in right side.   * Curvilinear thrombus in distal right MCA causing occlusion and non  visualisation of distal right MCA and right MCA distal branches.   * Chronic ischemic demylenation in bilateral posterior periventricular white  matter, left coro na radiata and centrum semiovale.   * Hyoplastic  left vertebral  artery.  Distal  left vertebral  artery  not well visualised   - ? Due to occlusion  / ? Very  slow  flow status.   * Focal  mild  stenosis  in basilar  artery  in P1 segment  of left PCA.       Consultant Radiologist\"\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "# Define the file path to the PDF file named \"MO.pdf\" in the current working directory\n",
    "file_path = r\"Data/MO.pdf\"  # The 'r' prefix indicates a raw string to handle any special characters in the path\n",
    "\n",
    "# Read the PDF file and extract its content as text\n",
    "print(\n",
    "    # Remove sensitive information like patient name, physician name, etc.\n",
    "    text_json(\n",
    "        remove_sensitive(\n",
    "            read_pdf(file_path)  # Read PDF and extract text content\n",
    "        )\n",
    "    )\n",
    ")\n",
    "# Convert the sanitized text to a structured JSON format and print it"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b0dc7e8-0e28-4e1e-814b-915ab8f613d8",
   "metadata": {},
   "source": [
    "### Processing multiple pdf file for chromadb format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6f63da21-07cb-4dab-b8ad-2bbe32cf0549",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data/MO.pdf\n",
      "Data/RA.pdf\n",
      "Data/VA.pdf\n"
     ]
    }
   ],
   "source": [
    "# Process all PDF files in the folder\n",
    "import os\n",
    "folder_path = r\"Data/\"\n",
    "all_data = []\n",
    "for filename in os.listdir(folder_path):\n",
    "    if filename.endswith(\".pdf\"):\n",
    "        pdf_path = os.path.join(folder_path, filename)\n",
    "        print(pdf_path)\n",
    "        extracted_data = read_pdf(pdf_path)\n",
    "        remove_sensitive_data = remove_sensitive(extracted_data)\n",
    "        extracted_json = text_json(remove_sensitive_data)\n",
    "        ## Parse the JSON strings into dictionaries and append to the list\n",
    "        all_data.append(json.loads(extracted_json))\n",
    "#print(all_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09459700-87bc-48bf-8a80-c4076b43f45a",
   "metadata": {},
   "source": [
    "### Converting JSON to ChromaDB format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0f0a90c7-523e-43d7-810d-a5cb13f95926",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.schema import Document\n",
    "\n",
    "documents = [\n",
    "    Document(page_content=f\"{item['Patient ID']} {item['Patient age']} {item['Sex']} {item['TECHNIQUE']} {item['FINDINGS']} {item['IMPRESSION']}\", metadata={})\n",
    "    for item in all_data\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c6493071-8e88-4121-a957-905ed5d3b433",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Page Content: MR0000444444 075Y M T2 FSE Axials  / Sagittals  & Coronals.   T1 &T2  FLAIR  Axials.  DWI,  GRE  Axials.    3D TOF  MR Angiography  of Intracranial  Arteries. Large wedge shaped lesion with restricted diffusion low ADC values and hyperintensities  in T2 & FLAIR images noted involving capsuloganglionic region, parietal lobe, adjacent  frontal insular cortex, temporal lobe in right side.   Focal FLAIR hyperintensities without restricted diffusion in bilateral posterior  periventricular wh ite matter, left corona radiata and centrum semiovale.   Curvilinear  blooming  SWI hypointensities  in distal  right  MCA.   Cerebellum, 4th ventricle, brain stem & CP angle regions are within normal limits.  Sella, suprasellar & parasellar areas are normal.   The ex tracerebral spaces and supratentorial ventricular system are normal.  Rest of the cerebral parenchyma is normal is normal.   Midline structures and corpus callosum are normal.  No haemorrhagic pathology.   No extraaxial  collection  seen.   MR Angiography  shows :  Hypoplastic  left vertebral  artery.   Distal  left vertebral  artery  not well visualised.   Focal  mild stenosis  in basilar  artery  in P1 segment  of left PCA.        Non visualisation  of distal  2/3rds  of right  MCA  and distal  right  MCA  branches.   Bilateral intracranial internal carotid arteries, anterior & left middle cerebral arteries  and their branches are normal in caliber/flow signal.   Otherwise basilar artery, bilateral posterior cerebral, superior cerebellar and anterior  inferior cerebellar arteries are normal in ca liber and course.   Right vertebral artery is normal in calibre and flow signal.  No aneurysm / AVM seen. * Large acute non haemorrhagic infract in right MCA territory involving  capsuloganglionic region,  parietal lobe, adjacent frontal insular corte x,  temporal lobe in right side.   * Curvilinear thrombus in distal right MCA causing occlusion and non  visualisation of distal right MCA and right MCA distal branches.   * Chronic ischemic demylenation in bilateral posterior periventricular white  matter, left coro na radiata and centrum semiovale.   * Hyoplastic  left vertebral  artery.  Distal  left vertebral  artery  not well visualised   - ? Due to occlusion  / ? Very  slow  flow status.   * Focal  mild  stenosis  in basilar  artery  in P1 segment  of left PCA.       Consultant Radiologist, Metadata: {}\n",
      "Page Content: MR0000222222 062Y M T2 FSE Axials  / Sagittals  & Coronals.   T1 &T2  FLAIR  Axials.  DWI,  GRE  Axials.   3D TOF  MR Angiography  of Intracranial  Arteries. Wedge shaped lesion showing restricted diffusion, low ADC values and slightly  hyperintense on FLAIR images involving right side parietal lobe, insular cortex, adjacent  frontal lobe and capsuloganglionic region.   Small lesions in parafalcine inferior both occipital lobes is hypointense on T1,  predominantly hypointense with adjacent hyperintensities on FLAIR and hyperintense on  T2 WI.   Lacunar FLAIR hyperintensities without restricted diffusion in right side anterior  corona  radiata.   Mega  cisterna  magna  seen - Normal  variant.   Cerebellum, 4th ventricle, brain stem & CP angle regions are within normal limits.  Sella, suprasellar & parasellar areas are normal.   The extracerebral spaces and supratentorial ventricular system are normal.  Rest of the cerebral parenchyma is normal.   Midline  structures  and corpus  callosum  are normal.   No haemorrhagic pathology. No extraaxial collection seen.  MR Angiography shows : (Patient is not cooperative)   Distal right MCA branches are not well visualized.   Small  stenotic  segment  in distal  left MCA.   Mild  stenosis  seen in cavernous  and infrapetrous  parts  of right  ICA.         Fetal  type of A1 segment  of right  ACA  -- Normal  varient.   Hypoplastic  right  vertebral  artery.   Left internal  carotid,  right  MCA  and rest of the left MCA  are normal  in caliber.   Bilateral intracranial anterior cerebral arteries and their branches are normal in caliber/flow  signal.   The basilar artery, bilateral posterior cerebral, superior cerebellar and anterior inferior  cerebellar arteries are norm al in caliber and course.   Left vertebral artery is normal in calibre and flow signal.  No aneurysm / AVM seen. * Non hemorrhagic hyperacute on acute infarct in right MCA territory,  involving right side parietal lobe, insular cortex, adjacent frontal lobe and  capsuloganglionic region.   * Small  lesions  of gliosis  / old infarcts  in parafalcine  inferior  both  occipital  lobes .  * Focal chronic infarct with residual chronic ischemic demyelination in right  side anterior corona radiata.   * Distal right MCA branches are not well visualized, possible due to slow flow  status / occlusion.   * Small  stenotic  segment  in distal  left MCA.   * Mild  stenosis  in cavernous  and infrapetrous  parts  of right  ICA.   * Hypoplastic  right  vertebral  artery.       Consultant Radiologist, Metadata: {}\n",
      "Page Content: IP0000111111 067Y M T1, T2 & FLAIR  Axials,T2  Saggitals  / Coronals.  DWI  Axials.   Clinical  details  : S/p CABG  for CAD. Evidence of multiple lacunar &  tiny sized lesions with restricted diffusion and low ADC  values are hyperintense on FLAIR images noted in both cerebellar hemispheres, occipital  lobes and parietooccipital regions (including right side of splenium of corpus callosum),  left posterior periv entricular white matter, left posterior temporal lobe, left frontal corona  radiata & centrum semiovale, left high parietal cortex and right side corona radiata,  centrum semiovale and high parietal cortex and as a tiny lesion in lateral wall of the   frontal horn of right lateral ventricle.   Rest of the cerebral  parenchyma  is normal.   Both  lateral  and third  ventricles  are mildly  dilated.   Bilateral sylvian fissures, cerebral sulcal spaces and basi cisternal spaces are widened.  Pons, medulla and midbrain are normal.   No shift of midline  structures.   Pituitary gland, infundibulum and optic chiasm are normal.  No hemorrhagic pathology.   Screening  MR Angio  : Normal  major  intracranial  arteries.    :04 * Multiple tiny & lacunar sized non hemorrhagic acute infarcts in bilateral  cerebellar hemispheres, occipital lobes &  parietooccipital regions (including  right side of splenium of corpus callosum), left sided posterior periventricular  white matter, posterior temporal lobe, frontal corona radiata, centrum  semiovale, high parietal cortex and right sided corona radiata, cen trum  semiovale, high parietal cortex and lateral wall of the frontal horn of right  lateral ventricle -- Embolic etiology.   * Age related  mild  cerebral  atrophy.       For clinical  correlation.         Consultant Radiologist, Metadata: {}\n"
     ]
    }
   ],
   "source": [
    "# Verify the result\n",
    "for doc in documents:\n",
    "    print(f\"Page Content: {doc.page_content}, Metadata: {doc.metadata}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fe219a7-37df-4a70-849f-a242ca30e457",
   "metadata": {},
   "source": [
    "### Creating chunks from document from chromaDB (vector database)\n",
    "### Using all-MiniLM-L6-v2 embedings\n",
    "### Saving the data into chromaDB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "675f842c-9e22-42b4-bab0-5c743f8597e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data has been successfully inserted into ChromaDB!\n"
     ]
    }
   ],
   "source": [
    "from langchain.vectorstores import Chroma\n",
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "\n",
    "# -------------------------------\n",
    "# Split Data into Chunks (Optional for Large Content)\n",
    "# -------------------------------\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=500, chunk_overlap=50)\n",
    "split_documents = text_splitter.split_documents(documents)\n",
    "\n",
    "# -------------------------------\n",
    "# Use Hugging Face Embeddings (all-MiniLM-L6-v2)\n",
    "# -------------------------------\n",
    "embedding_model_name = \"sentence-transformers/all-MiniLM-L6-v2\"\n",
    "embeddings = HuggingFaceEmbeddings(model_name=embedding_model_name)\n",
    "\n",
    "# -------------------------------\n",
    "# Store Embeddings in ChromaDB\n",
    "# -------------------------------\n",
    "chroma_db4 = Chroma.from_documents(split_documents, embeddings, persist_directory=\"./chroma_sample_db\")\n",
    "\n",
    "# Save embeddings to the ChromaDB directory\n",
    "chroma_db4.persist()\n",
    "print(\"Data has been successfully inserted into ChromaDB!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "cc95e354-2bec-49ec-a0cd-21c7b1da6693",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "client=SentenceTransformer(\n",
      "  (0): Transformer({'max_seq_length': 256, 'do_lower_case': False}) with Transformer model: BertModel \n",
      "  (1): Pooling({'word_embedding_dimension': 384, 'pooling_mode_cls_token': False, 'pooling_mode_mean_tokens': True, 'pooling_mode_max_tokens': False, 'pooling_mode_mean_sqrt_len_tokens': False, 'pooling_mode_weightedmean_tokens': False, 'pooling_mode_lasttoken': False, 'include_prompt': True})\n",
      "  (2): Normalize()\n",
      ") model_name='sentence-transformers/all-MiniLM-L6-v2' cache_folder=None model_kwargs={} encode_kwargs={} multi_process=False show_progress=False\n"
     ]
    }
   ],
   "source": [
    "print(embeddings)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70d5277d-1485-4333-8104-b9ef1b478080",
   "metadata": {},
   "source": [
    "### Test Queries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8224eb14-2b06-4452-8af8-5f9388c8ab0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def retrieve_queries(queries, k):\n",
    "    # Create a retriever instance from the Chroma database to retrieve relevant documents\n",
    "    retriever = chroma_db4.as_retriever()\n",
    "    # Loop through each query in the test_queries list\n",
    "    for query in queries:\n",
    "        # Retrieve the top k most relevant documents for the current query\n",
    "        results = retriever.get_relevant_documents(query, k=k)\n",
    "        \n",
    "        # Print the current query being tested\n",
    "        print(f\" Query: {query}\")\n",
    "        \n",
    "        # Loop through the top k results and display their content\n",
    "        for i, doc in enumerate(results[:k]):  # Limit to top k results\n",
    "            print(f\" Result {i+1}: {doc.page_content}\")\n",
    "        \n",
    "        # Print a separator line for better readability between results\n",
    "        print(\"-\" * 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ed8b6a57-8bac-4e64-8d82-b892e198e84f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\bhowm\\.conda\\envs\\new1\\Lib\\site-packages\\langchain_core\\_api\\deprecation.py:139: LangChainDeprecationWarning: The method `BaseRetriever.get_relevant_documents` was deprecated in langchain-core 0.1.46 and will be removed in 0.3.0. Use invoke instead.\n",
      "  warn_deprecated(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Query: Patient is not cooperative\n",
      " Result 1: are normal.  Rest of the cerebral parenchyma is normal.   Midline  structures  and corpus  callosum  are normal.   No haemorrhagic pathology. No extraaxial collection seen.  MR Angiography shows : (Patient is not cooperative)   Distal right MCA branches are not well visualized.   Small  stenotic  segment  in distal  left MCA.   Mild  stenosis  seen in cavernous  and infrapetrous  parts  of right  ICA.         Fetal  type of A1 segment  of right  ACA  -- Normal  varient.   Hypoplastic  right\n",
      "--------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# Define a list of test queries to be processed\n",
    "queries = [\"Patient is not cooperative\"]\n",
    "k = 1\n",
    "retrieve_queries(queries, k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "01331cbb-866c-4d68-9465-a7c2190d04ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Query: including right side of splenium of corpus callosum\n",
      " Result 1: IP0000111111 067Y M T1, T2 & FLAIR  Axials,T2  Saggitals  / Coronals.  DWI  Axials.   Clinical  details  : S/p CABG  for CAD. Evidence of multiple lacunar &  tiny sized lesions with restricted diffusion and low ADC  values are hyperintense on FLAIR images noted in both cerebellar hemispheres, occipital  lobes and parietooccipital regions (including right side of splenium of corpus callosum),  left posterior periv entricular white matter, left posterior temporal lobe, left frontal corona  radiata\n",
      " Result 2: are normal.  Rest of the cerebral parenchyma is normal.   Midline  structures  and corpus  callosum  are normal.   No haemorrhagic pathology. No extraaxial collection seen.  MR Angiography shows : (Patient is not cooperative)   Distal right MCA branches are not well visualized.   Small  stenotic  segment  in distal  left MCA.   Mild  stenosis  seen in cavernous  and infrapetrous  parts  of right  ICA.         Fetal  type of A1 segment  of right  ACA  -- Normal  varient.   Hypoplastic  right\n",
      "--------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# Define a list of test queries to be processed\n",
    "queries = [\"including right side of splenium of corpus callosum\"]\n",
    "k = 2\n",
    "retrieve_queries(queries, k)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79ff10f0-2c63-448a-b20d-530940e25f8c",
   "metadata": {},
   "source": [
    "# bleu_score"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce1a62af-4de5-45c8-907c-0efd6849a0be",
   "metadata": {},
   "source": [
    "BLEU (Bilingual Evaluation Understudy) is a metric for evaluating the quality of machine-translated text by comparing it to a reference text. It measures n-gram overlap (word sequences) between the generated and reference texts.\n",
    "\n",
    "\n",
    "BLEU Score\tInterpretation\n",
    "0.9 - 1.0\tPerfect translation\n",
    "0.7 - 0.9\tHigh-quality translation\n",
    "0.4 - 0.7\tModerate quality\n",
    "0.1 - 0.4\tWeak translation\n",
    "0.0 - 0.1\tPoor match\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "6a91fc0b-9b4a-4b98-9fc0-e94b71947c85",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting nltk\n",
      "  Using cached nltk-3.9.1-py3-none-any.whl.metadata (2.9 kB)\n",
      "Requirement already satisfied: click in c:\\users\\bhowm\\appdata\\roaming\\python\\python312\\site-packages (from nltk) (8.1.7)\n",
      "Requirement already satisfied: joblib in c:\\users\\bhowm\\.conda\\envs\\new1\\lib\\site-packages (from nltk) (1.4.2)\n",
      "Requirement already satisfied: regex>=2021.8.3 in c:\\users\\bhowm\\.conda\\envs\\new1\\lib\\site-packages (from nltk) (2024.11.6)\n",
      "Requirement already satisfied: tqdm in c:\\users\\bhowm\\.conda\\envs\\new1\\lib\\site-packages (from nltk) (4.67.1)\n",
      "Requirement already satisfied: colorama in c:\\users\\bhowm\\.conda\\envs\\new1\\lib\\site-packages (from click->nltk) (0.4.6)\n",
      "Using cached nltk-3.9.1-py3-none-any.whl (1.5 MB)\n",
      "Installing collected packages: nltk\n",
      "Successfully installed nltk-3.9.1\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "0229b54b-106a-4db3-8a22-b1debec4c4c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Result 1: are normal.  Rest of the cerebral parenchyma is normal.   Midline  structures  and corpus  callosum  are normal.   No haemorrhagic pathology. No extraaxial collection seen.  MR Angiography shows : (Patient is not cooperative)   Distal right MCA branches are not well visualized.   Small  stenotic  segment  in distal  left MCA.   Mild  stenosis  seen in cavernous  and infrapetrous  parts  of right  ICA.         Fetal  type of A1 segment  of right  ACA  -- Normal  varient.   Hypoplastic  right\n",
      " BLEU Score: 0.0278\n",
      "--------------------------------------------------\n",
      " Result 2: MCA branches are not well visualized, possible due to slow flow  status / occlusion.   * Small  stenotic  segment  in distal  left MCA.   * Mild  stenosis  in cavernous  and infrapetrous  parts  of right  ICA.   * Hypoplastic  right  vertebral  artery.       Consultant Radiologist\n",
      " BLEU Score: 0.0041\n",
      "--------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\bhowm\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.translate.bleu_score import sentence_bleu, SmoothingFunction\n",
    "\n",
    "# Ensure NLTK components are available\n",
    "nltk.download('punkt')\n",
    "\n",
    "# Define a reference document (ground truth)\n",
    "reference_text = \"Patient is not cooperative\"\n",
    "reference_tokens = [nltk.word_tokenize(reference_text.lower())]  # Tokenize & lowercase\n",
    "\n",
    "# Test Query\n",
    "test_query = \"Patient is non-cooperative\"\n",
    "\n",
    "# Retrieve relevant documents\n",
    "retriever = chroma_db4.as_retriever()\n",
    "results = retriever.get_relevant_documents(test_query, k=2)\n",
    "\n",
    "# Compute BLEU Score for each retrieved document\n",
    "for i, doc in enumerate(results[:2]):  # Top 2 results\n",
    "    candidate_tokens = nltk.word_tokenize(doc.page_content.lower())  # Tokenize candidate\n",
    "    bleu_score = sentence_bleu(reference_tokens, candidate_tokens, \n",
    "                               smoothing_function=SmoothingFunction().method1)\n",
    "\n",
    "    print(f\" Result {i+1}: {doc.page_content}\")\n",
    "    print(f\" BLEU Score: {bleu_score:.4f}\")  # Print BLEU score\n",
    "    print(\"-\" * 50)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "804861b3-874d-48fc-bf35-61b582cc3d50",
   "metadata": {},
   "source": [
    "# rouge_scorer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "941f5375-72f3-46d1-9c5d-ce7b6b6d544c",
   "metadata": {},
   "source": [
    "ROUGE (Recall-Oriented Understudy for Gisting Evaluation) is a set of metrics used to evaluate text summarization and machine-generated content by comparing it to a reference text. Unlike BLEU (which focuses on precision), ROUGE prioritizes recall, measuring how much of the reference text appears in the generated text.\n",
    "\n",
    "\n",
    "ROUGE Score Interpretation\n",
    "ROUGE Score\tInterpretation\n",
    "0.8 - 1.0\tExcellent summary\n",
    "0.6 - 0.8\tGood summary\n",
    "0.4 - 0.6\tModerate similarity\n",
    "0.2 - 0.4\tWeak match\n",
    "0.0 - 0.2\tPoor summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "f1333839-9081-456c-b0dd-d96363d5fdce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting rouge-scoreNote: you may need to restart the kernel to use updated packages.\n",
      "\n",
      "  Downloading rouge_score-0.1.2.tar.gz (17 kB)\n",
      "  Preparing metadata (setup.py): started\n",
      "  Preparing metadata (setup.py): finished with status 'done'\n",
      "Requirement already satisfied: nltk in c:\\users\\bhowm\\.conda\\envs\\new1\\lib\\site-packages (3.9.1)\n",
      "Collecting absl-py (from rouge-score)\n",
      "  Downloading absl_py-2.1.0-py3-none-any.whl.metadata (2.3 kB)\n",
      "Requirement already satisfied: numpy in c:\\users\\bhowm\\.conda\\envs\\new1\\lib\\site-packages (from rouge-score) (1.26.4)\n",
      "Requirement already satisfied: six>=1.14.0 in c:\\users\\bhowm\\.conda\\envs\\new1\\lib\\site-packages (from rouge-score) (1.16.0)\n",
      "Requirement already satisfied: click in c:\\users\\bhowm\\appdata\\roaming\\python\\python312\\site-packages (from nltk) (8.1.7)\n",
      "Requirement already satisfied: joblib in c:\\users\\bhowm\\.conda\\envs\\new1\\lib\\site-packages (from nltk) (1.4.2)\n",
      "Requirement already satisfied: regex>=2021.8.3 in c:\\users\\bhowm\\.conda\\envs\\new1\\lib\\site-packages (from nltk) (2024.11.6)\n",
      "Requirement already satisfied: tqdm in c:\\users\\bhowm\\.conda\\envs\\new1\\lib\\site-packages (from nltk) (4.67.1)\n",
      "Requirement already satisfied: colorama in c:\\users\\bhowm\\.conda\\envs\\new1\\lib\\site-packages (from click->nltk) (0.4.6)\n",
      "Downloading absl_py-2.1.0-py3-none-any.whl (133 kB)\n",
      "Building wheels for collected packages: rouge-score\n",
      "  Building wheel for rouge-score (setup.py): started\n",
      "  Building wheel for rouge-score (setup.py): finished with status 'done'\n",
      "  Created wheel for rouge-score: filename=rouge_score-0.1.2-py3-none-any.whl size=24972 sha256=f7398977d5c58eae4abc4d44709d2ac91c5f348596530b43aeb9ea989707d827\n",
      "  Stored in directory: c:\\users\\bhowm\\appdata\\local\\pip\\cache\\wheels\\85\\9d\\af\\01feefbe7d55ef5468796f0c68225b6788e85d9d0a281e7a70\n",
      "Successfully built rouge-score\n",
      "Installing collected packages: absl-py, rouge-score\n",
      "Successfully installed absl-py-2.1.0 rouge-score-0.1.2\n"
     ]
    }
   ],
   "source": [
    "pip install rouge-score nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "b6c3ece6-5128-4b19-8052-3aa8dd726d04",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Result 1: are normal.  Rest of the cerebral parenchyma is normal.   Midline  structures  and corpus  callosum  are normal.   No haemorrhagic pathology. No extraaxial collection seen.  MR Angiography shows : (Patient is not cooperative)   Distal right MCA branches are not well visualized.   Small  stenotic  segment  in distal  left MCA.   Mild  stenosis  seen in cavernous  and infrapetrous  parts  of right  ICA.         Fetal  type of A1 segment  of right  ACA  -- Normal  varient.   Hypoplastic  right\n",
      " ROUGE-1 (Unigrams) Recall: 1.0000, Precision: 0.0588, F1: 0.1111\n",
      " ROUGE-2 (Bigrams) Recall: 1.0000, Precision: 0.0448, F1: 0.0857\n",
      " ROUGE-L (Longest Common Subsequence) Recall: 1.0000, Precision: 0.0588, F1: 0.1111\n",
      "--------------------------------------------------\n",
      " Result 2: MCA branches are not well visualized, possible due to slow flow  status / occlusion.   * Small  stenotic  segment  in distal  left MCA.   * Mild  stenosis  in cavernous  and infrapetrous  parts  of right  ICA.   * Hypoplastic  right  vertebral  artery.       Consultant Radiologist\n",
      " ROUGE-1 (Unigrams) Recall: 0.2500, Precision: 0.0278, F1: 0.0500\n",
      " ROUGE-2 (Bigrams) Recall: 0.0000, Precision: 0.0000, F1: 0.0000\n",
      " ROUGE-L (Longest Common Subsequence) Recall: 0.2500, Precision: 0.0278, F1: 0.0500\n",
      "--------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from rouge_score import rouge_scorer\n",
    "\n",
    "# Define a reference document (ground truth)\n",
    "reference_text = \"Patient is not cooperative\"\n",
    "\n",
    "# Test Query\n",
    "test_query = \"Patient is non-cooperative\"\n",
    "\n",
    "# Retrieve relevant documents\n",
    "retriever = chroma_db4.as_retriever()\n",
    "results = retriever.get_relevant_documents(test_query, k=2)\n",
    "\n",
    "# Initialize ROUGE scorer\n",
    "scorer = rouge_scorer.RougeScorer(['rouge1', 'rouge2', 'rougeL'], use_stemmer=True)\n",
    "\n",
    "# Compute ROUGE Score for each retrieved document\n",
    "for i, doc in enumerate(results[:2]):  # Top 2 results\n",
    "    scores = scorer.score(reference_text, doc.page_content)\n",
    "\n",
    "    print(f\" Result {i+1}: {doc.page_content}\")\n",
    "    print(f\" ROUGE-1 (Unigrams) Recall: {scores['rouge1'].recall:.4f}, Precision: {scores['rouge1'].precision:.4f}, F1: {scores['rouge1'].fmeasure:.4f}\")\n",
    "    print(f\" ROUGE-2 (Bigrams) Recall: {scores['rouge2'].recall:.4f}, Precision: {scores['rouge2'].precision:.4f}, F1: {scores['rouge2'].fmeasure:.4f}\")\n",
    "    print(f\" ROUGE-L (Longest Common Subsequence) Recall: {scores['rougeL'].recall:.4f}, Precision: {scores['rougeL'].precision:.4f}, F1: {scores['rougeL'].fmeasure:.4f}\")\n",
    "    print(\"-\" * 50)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f8702cb-aa5c-4282-8c52-617805020460",
   "metadata": {},
   "source": [
    "# METEOR Score"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdb05096-3132-4328-a6b0-887072e7d6f3",
   "metadata": {},
   "source": [
    "METEOR (Metric for Evaluation of Translation with Explicit ORdering) is a text evaluation metric designed to improve upon BLEU and ROUGE by considering:\n",
    "Synonyms & Stemming (word variations count as matches)\n",
    "Word Order (penalizes incorrect word sequence)\n",
    "Recall & Precision Balance (avoids BLEUs bias toward precision)\n",
    "\n",
    "METEOR Score\tInterpretation\n",
    "0.9 - 1.0\tPerfect match\n",
    "0.7 - 0.9\tHigh similarity\n",
    "0.4 - 0.7\tModerate match\n",
    "0.2 - 0.4\tWeak similarity\n",
    "0.0 - 0.2\tPoor match\n",
    "\n",
    "\n",
    "METEOR vs BLEU vs ROUGE\n",
    "Metric\tStrengths\tWeaknesses\n",
    "BLEU\tGood for machine translation\tIgnores synonyms, sensitive to word order\n",
    "ROUGE\tGood for summarization\tDoesn't account for meaning\n",
    "METEOR\tBest for paraphrased sentences\tSlightly slower computation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "4ee5975a-1460-492c-a589-69b5fc86bb7d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\bhowm\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\bhowm\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Result 1: are normal.  Rest of the cerebral parenchyma is normal.   Midline  structures  and corpus  callosum  are normal.   No haemorrhagic pathology. No extraaxial collection seen.  MR Angiography shows : (Patient is not cooperative)   Distal right MCA branches are not well visualized.   Small  stenotic  segment  in distal  left MCA.   Mild  stenosis  seen in cavernous  and infrapetrous  parts  of right  ICA.         Fetal  type of A1 segment  of right  ACA  -- Normal  varient.   Hypoplastic  right\n",
      " METEOR Score: 0.2698\n",
      "--------------------------------------------------\n",
      " Result 2: MCA branches are not well visualized, possible due to slow flow  status / occlusion.   * Small  stenotic  segment  in distal  left MCA.   * Mild  stenosis  in cavernous  and infrapetrous  parts  of right  ICA.   * Hypoplastic  right  vertebral  artery.       Consultant Radiologist\n",
      " METEOR Score: 0.0617\n",
      "--------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.translate.meteor_score import meteor_score\n",
    "\n",
    "# Ensure NLTK components are available\n",
    "nltk.download('wordnet')\n",
    "nltk.download('punkt')\n",
    "\n",
    "# Define a reference document (ground truth)\n",
    "reference_text = \"Patient is not cooperative\"\n",
    "reference_tokens = nltk.word_tokenize(reference_text.lower())  # Tokenize & lowercase\n",
    "\n",
    "# Test Query\n",
    "test_query = \"Patient is non-cooperative\"\n",
    "\n",
    "# Retrieve relevant documents\n",
    "retriever = chroma_db4.as_retriever()\n",
    "results = retriever.get_relevant_documents(test_query, k=2)\n",
    "\n",
    "# Compute METEOR Score for each retrieved document\n",
    "for i, doc in enumerate(results[:2]):  # Top 2 results\n",
    "    candidate_tokens = nltk.word_tokenize(doc.page_content.lower())  # Tokenize candidate\n",
    "    meteor = meteor_score([reference_tokens], candidate_tokens)  # Compute METEOR\n",
    "\n",
    "    print(f\" Result {i+1}: {doc.page_content}\")\n",
    "    print(f\" METEOR Score: {meteor:.4f}\")  # Print METEOR score\n",
    "    print(\"-\" * 50)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec99ea01-ebd5-4cd6-b096-c763c88ec90b",
   "metadata": {},
   "source": [
    "# bert-score"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e203b4a0-7adc-4e2f-b5f1-4ec8439eea5d",
   "metadata": {},
   "source": [
    "BERTScore is a modern text evaluation metric that compares deep semantic meaning rather than just word overlap. It uses BERT (or similar Transformer models) to generate word embeddings and calculates similarity between the generated and reference texts.\n",
    "\n",
    "BERTScore Interpretation\n",
    "BERTScore\tInterpretation\n",
    "0.9 - 1.0\tAlmost identical\n",
    "0.7 - 0.9\tStrong similarity\n",
    "0.4 - 0.7\tModerate match\n",
    "0.2 - 0.4\tWeak similarity\n",
    "0.0 - 0.2\tPoor match\n",
    "\n",
    "\n",
    "BERTScore vs BLEU vs ROUGE\n",
    "Metric\tStrengths\tWeaknesses\n",
    "BLEU\tGood for exact word overlap\tFails on paraphrased text\n",
    "ROUGE\tGood for summarization\tIgnores meaning differences\n",
    "BERTScore\tCaptures semantic meaning & synonyms\tRequires deep learning models (slower)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "c931929d-a5ca-4c16-ab72-2a44391eebd7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting bert-score\n",
      "  Downloading bert_score-0.3.13-py3-none-any.whl.metadata (15 kB)\n",
      "Requirement already satisfied: torch>=1.0.0 in c:\\users\\bhowm\\.conda\\envs\\new1\\lib\\site-packages (from bert-score) (2.5.1)\n",
      "Requirement already satisfied: pandas>=1.0.1 in c:\\users\\bhowm\\appdata\\roaming\\python\\python312\\site-packages (from bert-score) (2.2.3)\n",
      "Requirement already satisfied: transformers>=3.0.0 in c:\\users\\bhowm\\.conda\\envs\\new1\\lib\\site-packages (from bert-score) (4.47.1)\n",
      "Requirement already satisfied: numpy in c:\\users\\bhowm\\.conda\\envs\\new1\\lib\\site-packages (from bert-score) (1.26.4)\n",
      "Requirement already satisfied: requests in c:\\users\\bhowm\\.conda\\envs\\new1\\lib\\site-packages (from bert-score) (2.32.3)\n",
      "Requirement already satisfied: tqdm>=4.31.1 in c:\\users\\bhowm\\.conda\\envs\\new1\\lib\\site-packages (from bert-score) (4.67.1)\n",
      "Collecting matplotlib (from bert-score)\n",
      "  Downloading matplotlib-3.10.0-cp312-cp312-win_amd64.whl.metadata (11 kB)\n",
      "Requirement already satisfied: packaging>=20.9 in c:\\users\\bhowm\\.conda\\envs\\new1\\lib\\site-packages (from bert-score) (24.2)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\bhowm\\.conda\\envs\\new1\\lib\\site-packages (from pandas>=1.0.1->bert-score) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\bhowm\\.conda\\envs\\new1\\lib\\site-packages (from pandas>=1.0.1->bert-score) (2024.1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in c:\\users\\bhowm\\appdata\\roaming\\python\\python312\\site-packages (from pandas>=1.0.1->bert-score) (2024.2)\n",
      "Requirement already satisfied: filelock in c:\\users\\bhowm\\.conda\\envs\\new1\\lib\\site-packages (from torch>=1.0.0->bert-score) (3.16.1)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in c:\\users\\bhowm\\.conda\\envs\\new1\\lib\\site-packages (from torch>=1.0.0->bert-score) (4.12.2)\n",
      "Requirement already satisfied: networkx in c:\\users\\bhowm\\.conda\\envs\\new1\\lib\\site-packages (from torch>=1.0.0->bert-score) (3.4.2)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\bhowm\\.conda\\envs\\new1\\lib\\site-packages (from torch>=1.0.0->bert-score) (3.1.4)\n",
      "Requirement already satisfied: fsspec in c:\\users\\bhowm\\.conda\\envs\\new1\\lib\\site-packages (from torch>=1.0.0->bert-score) (2024.12.0)\n",
      "Requirement already satisfied: setuptools in c:\\users\\bhowm\\.conda\\envs\\new1\\lib\\site-packages (from torch>=1.0.0->bert-score) (75.1.0)\n",
      "Requirement already satisfied: sympy==1.13.1 in c:\\users\\bhowm\\.conda\\envs\\new1\\lib\\site-packages (from torch>=1.0.0->bert-score) (1.13.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in c:\\users\\bhowm\\.conda\\envs\\new1\\lib\\site-packages (from sympy==1.13.1->torch>=1.0.0->bert-score) (1.3.0)\n",
      "Requirement already satisfied: colorama in c:\\users\\bhowm\\.conda\\envs\\new1\\lib\\site-packages (from tqdm>=4.31.1->bert-score) (0.4.6)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.24.0 in c:\\users\\bhowm\\.conda\\envs\\new1\\lib\\site-packages (from transformers>=3.0.0->bert-score) (0.27.1)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\bhowm\\.conda\\envs\\new1\\lib\\site-packages (from transformers>=3.0.0->bert-score) (6.0.2)\n",
      "Requirement already satisfied: regex!=2019.12.17 in c:\\users\\bhowm\\.conda\\envs\\new1\\lib\\site-packages (from transformers>=3.0.0->bert-score) (2024.11.6)\n",
      "Requirement already satisfied: tokenizers<0.22,>=0.21 in c:\\users\\bhowm\\appdata\\roaming\\python\\python312\\site-packages (from transformers>=3.0.0->bert-score) (0.21.0)\n",
      "Requirement already satisfied: safetensors>=0.4.1 in c:\\users\\bhowm\\.conda\\envs\\new1\\lib\\site-packages (from transformers>=3.0.0->bert-score) (0.5.0)\n",
      "Collecting contourpy>=1.0.1 (from matplotlib->bert-score)\n",
      "  Downloading contourpy-1.3.1-cp312-cp312-win_amd64.whl.metadata (5.4 kB)\n",
      "Collecting cycler>=0.10 (from matplotlib->bert-score)\n",
      "  Downloading cycler-0.12.1-py3-none-any.whl.metadata (3.8 kB)\n",
      "Collecting fonttools>=4.22.0 (from matplotlib->bert-score)\n",
      "  Downloading fonttools-4.55.7-cp312-cp312-win_amd64.whl.metadata (102 kB)\n",
      "Collecting kiwisolver>=1.3.1 (from matplotlib->bert-score)\n",
      "  Downloading kiwisolver-1.4.8-cp312-cp312-win_amd64.whl.metadata (6.3 kB)\n",
      "Requirement already satisfied: pillow>=8 in c:\\users\\bhowm\\.conda\\envs\\new1\\lib\\site-packages (from matplotlib->bert-score) (11.1.0)\n",
      "Collecting pyparsing>=2.3.1 (from matplotlib->bert-score)\n",
      "  Downloading pyparsing-3.2.1-py3-none-any.whl.metadata (5.0 kB)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\bhowm\\.conda\\envs\\new1\\lib\\site-packages (from requests->bert-score) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\bhowm\\.conda\\envs\\new1\\lib\\site-packages (from requests->bert-score) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\bhowm\\.conda\\envs\\new1\\lib\\site-packages (from requests->bert-score) (2.2.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\bhowm\\.conda\\envs\\new1\\lib\\site-packages (from requests->bert-score) (2024.12.14)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\bhowm\\.conda\\envs\\new1\\lib\\site-packages (from python-dateutil>=2.8.2->pandas>=1.0.1->bert-score) (1.16.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\bhowm\\.conda\\envs\\new1\\lib\\site-packages (from jinja2->torch>=1.0.0->bert-score) (2.1.3)\n",
      "Downloading bert_score-0.3.13-py3-none-any.whl (61 kB)\n",
      "Downloading matplotlib-3.10.0-cp312-cp312-win_amd64.whl (8.0 MB)\n",
      "   ---------------------------------------- 0.0/8.0 MB ? eta -:--:--\n",
      "   ------------------ --------------------- 3.7/8.0 MB 21.8 MB/s eta 0:00:01\n",
      "   ------------------ --------------------- 3.7/8.0 MB 21.8 MB/s eta 0:00:01\n",
      "   ---------------------------- ----------- 5.8/8.0 MB 9.5 MB/s eta 0:00:01\n",
      "   --------------------------------- ------ 6.8/8.0 MB 8.4 MB/s eta 0:00:01\n",
      "   ---------------------------------------  7.9/8.0 MB 7.9 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 8.0/8.0 MB 6.6 MB/s eta 0:00:00\n",
      "Downloading contourpy-1.3.1-cp312-cp312-win_amd64.whl (220 kB)\n",
      "Downloading cycler-0.12.1-py3-none-any.whl (8.3 kB)\n",
      "Downloading fonttools-4.55.7-cp312-cp312-win_amd64.whl (2.2 MB)\n",
      "   ---------------------------------------- 0.0/2.2 MB ? eta -:--:--\n",
      "   ----------------------- ---------------- 1.3/2.2 MB 8.4 MB/s eta 0:00:01\n",
      "   -------------------------------------- - 2.1/2.2 MB 7.8 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 2.2/2.2 MB 4.4 MB/s eta 0:00:00\n",
      "Downloading kiwisolver-1.4.8-cp312-cp312-win_amd64.whl (71 kB)\n",
      "Downloading pyparsing-3.2.1-py3-none-any.whl (107 kB)\n",
      "Installing collected packages: pyparsing, kiwisolver, fonttools, cycler, contourpy, matplotlib, bert-score\n",
      "Successfully installed bert-score-0.3.13 contourpy-1.3.1 cycler-0.12.1 fonttools-4.55.7 kiwisolver-1.4.8 matplotlib-3.10.0 pyparsing-3.2.1\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install bert-score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "ecd57a84-b907-411c-90a6-c57ed0f119dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\bhowm\\.conda\\envs\\new1\\Lib\\site-packages\\huggingface_hub\\file_download.py:140: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\bhowm\\.cache\\huggingface\\hub\\models--microsoft--deberta-xlarge-mnli. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Result 1: are normal.  Rest of the cerebral parenchyma is normal.   Midline  structures  and corpus  callosum  are normal.   No haemorrhagic pathology. No extraaxial collection seen.  MR Angiography shows : (Patient is not cooperative)   Distal right MCA branches are not well visualized.   Small  stenotic  segment  in distal  left MCA.   Mild  stenosis  seen in cavernous  and infrapetrous  parts  of right  ICA.         Fetal  type of A1 segment  of right  ACA  -- Normal  varient.   Hypoplastic  right\n",
      " BERTScore Precision: 0.3465, Recall: 0.7349, F1 Score: 0.4710\n",
      "--------------------------------------------------\n",
      " Result 2: MCA branches are not well visualized, possible due to slow flow  status / occlusion.   * Small  stenotic  segment  in distal  left MCA.   * Mild  stenosis  in cavernous  and infrapetrous  parts  of right  ICA.   * Hypoplastic  right  vertebral  artery.       Consultant Radiologist\n",
      " BERTScore Precision: 0.3286, Recall: 0.5799, F1 Score: 0.4195\n",
      "--------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from bert_score import score\n",
    "\n",
    "# Define reference document (ground truth)\n",
    "reference_text = \"Patient is not cooperative\"\n",
    "\n",
    "# Test Query\n",
    "test_query = \"Patient is non-cooperative\"\n",
    "\n",
    "# Retrieve relevant documents\n",
    "retriever = chroma_db4.as_retriever()\n",
    "results = retriever.get_relevant_documents(test_query, k=2)\n",
    "\n",
    "# Compute BERTScore for each retrieved document\n",
    "for i, doc in enumerate(results[:2]):  # Top 2 results\n",
    "    candidate_text = doc.page_content  # Retrieved text\n",
    "\n",
    "    # Compute BERTScore (Precision, Recall, F1)\n",
    "    P, R, F1 = score([candidate_text], [reference_text], lang=\"en\", model_type=\"microsoft/deberta-xlarge-mnli\")\n",
    "\n",
    "    print(f\" Result {i+1}: {doc.page_content}\")\n",
    "    print(f\" BERTScore Precision: {P.item():.4f}, Recall: {R.item():.4f}, F1 Score: {F1.item():.4f}\")\n",
    "    print(\"-\" * 50)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "983c938c-4b2e-49f0-9484-f3593b670270",
   "metadata": {},
   "source": [
    "## Summerization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "92f72c9c-dd75-4fdf-a46a-56f74dd91591",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting ollama\n",
      "  Downloading ollama-0.4.7-py3-none-any.whl.metadata (4.7 kB)\n",
      "Requirement already satisfied: httpx<0.29,>=0.27 in c:\\users\\bhowm\\.conda\\envs\\new1\\lib\\site-packages (from ollama) (0.27.0)\n",
      "Collecting pydantic<3.0.0,>=2.9.0 (from ollama)\n",
      "  Downloading pydantic-2.10.6-py3-none-any.whl.metadata (30 kB)\n",
      "Requirement already satisfied: anyio in c:\\users\\bhowm\\.conda\\envs\\new1\\lib\\site-packages (from httpx<0.29,>=0.27->ollama) (4.6.2)\n",
      "Requirement already satisfied: certifi in c:\\users\\bhowm\\.conda\\envs\\new1\\lib\\site-packages (from httpx<0.29,>=0.27->ollama) (2024.12.14)\n",
      "Requirement already satisfied: httpcore==1.* in c:\\users\\bhowm\\.conda\\envs\\new1\\lib\\site-packages (from httpx<0.29,>=0.27->ollama) (1.0.2)\n",
      "Requirement already satisfied: idna in c:\\users\\bhowm\\.conda\\envs\\new1\\lib\\site-packages (from httpx<0.29,>=0.27->ollama) (3.7)\n",
      "Requirement already satisfied: sniffio in c:\\users\\bhowm\\.conda\\envs\\new1\\lib\\site-packages (from httpx<0.29,>=0.27->ollama) (1.3.0)\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in c:\\users\\bhowm\\.conda\\envs\\new1\\lib\\site-packages (from httpcore==1.*->httpx<0.29,>=0.27->ollama) (0.14.0)\n",
      "Collecting annotated-types>=0.6.0 (from pydantic<3.0.0,>=2.9.0->ollama)\n",
      "  Downloading annotated_types-0.7.0-py3-none-any.whl.metadata (15 kB)\n",
      "Collecting pydantic-core==2.27.2 (from pydantic<3.0.0,>=2.9.0->ollama)\n",
      "  Using cached pydantic_core-2.27.2-cp312-cp312-win_amd64.whl.metadata (6.7 kB)\n",
      "Requirement already satisfied: typing-extensions>=4.12.2 in c:\\users\\bhowm\\.conda\\envs\\new1\\lib\\site-packages (from pydantic<3.0.0,>=2.9.0->ollama) (4.12.2)\n",
      "Downloading ollama-0.4.7-py3-none-any.whl (13 kB)\n",
      "Downloading pydantic-2.10.6-py3-none-any.whl (431 kB)\n",
      "Using cached pydantic_core-2.27.2-cp312-cp312-win_amd64.whl (2.0 MB)\n",
      "Downloading annotated_types-0.7.0-py3-none-any.whl (13 kB)\n",
      "Installing collected packages: pydantic-core, annotated-types, pydantic, ollama\n",
      "  Attempting uninstall: pydantic\n",
      "    Found existing installation: pydantic 1.10.19\n",
      "    Uninstalling pydantic-1.10.19:\n",
      "      Successfully uninstalled pydantic-1.10.19\n",
      "Successfully installed annotated-types-0.7.0 ollama-0.4.7 pydantic-2.10.6 pydantic-core-2.27.2\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  WARNING: Failed to remove contents in a temporary directory 'C:\\Users\\bhowm\\.conda\\envs\\new1\\Lib\\site-packages\\~ydantic'.\n",
      "  You can safely remove it manually.\n",
      "ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "chromadb 0.4.0 requires pydantic<2.0,>=1.9, but you have pydantic 2.10.6 which is incompatible.\n",
      "fastapi 0.99.1 requires pydantic!=1.8,!=1.8.1,<2.0.0,>=1.7.4, but you have pydantic 2.10.6 which is incompatible.\n"
     ]
    }
   ],
   "source": [
    "pip install ollama"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "539c053d-d759-4f6d-9938-d4c163b58bea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pydantic in c:\\users\\bhowm\\.conda\\envs\\new1\\lib\\site-packages (2.10.6)Note: you may need to restart the kernel to use updated packages.\n",
      "\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in c:\\users\\bhowm\\.conda\\envs\\new1\\lib\\site-packages (from pydantic) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.27.2 in c:\\users\\bhowm\\.conda\\envs\\new1\\lib\\site-packages (from pydantic) (2.27.2)\n",
      "Requirement already satisfied: typing-extensions>=4.12.2 in c:\\users\\bhowm\\.conda\\envs\\new1\\lib\\site-packages (from pydantic) (4.12.2)\n"
     ]
    }
   ],
   "source": [
    "pip install --upgrade pydantic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "648cc7a1-3e1e-4605-92ed-61eae6a1dd3c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found existing installation: pydantic 1.10.13\n",
      "Uninstalling pydantic-1.10.13:\n",
      "  Successfully uninstalled pydantic-1.10.13\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip uninstall pydantic -y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5e168e1d-7529-4cce-af3c-ef3ea4e69af8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting pydantic==1.10.13Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "ollama 0.4.7 requires pydantic<3.0.0,>=2.9.0, but you have pydantic 1.10.13 which is incompatible.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "  Using cached pydantic-1.10.13-py3-none-any.whl.metadata (149 kB)\n",
      "Requirement already satisfied: typing-extensions>=4.2.0 in c:\\users\\bhowm\\.conda\\envs\\new1\\lib\\site-packages (from pydantic==1.10.13) (4.12.2)\n",
      "Using cached pydantic-1.10.13-py3-none-any.whl (158 kB)\n",
      "Installing collected packages: pydantic\n",
      "Successfully installed pydantic-1.10.13\n"
     ]
    }
   ],
   "source": [
    "pip install pydantic==1.10.13"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a13587d7-de3e-42ae-85cb-490d30cee8b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import ollama\n",
    "\n",
    "def summarize_medical_report(question, answer):\n",
    "    prompt = f\"\"\"\n",
    "    Given the medical report and patient condition:\n",
    "\n",
    "    Question: \"{question}\"\n",
    "    Answer: \"{answer}\"\n",
    "\n",
    "    **Task:** Summarize the key medical findings, focusing on abnormalities. If the patient was non-cooperative, mention any limitations in the findings.\n",
    "\n",
    "    **Output Format:** \n",
    "    - Key abnormalities only\n",
    "    - Impact of patient non-cooperation\n",
    "    - Concise and medically relevant summary\n",
    "\n",
    "    **Example Output:**\n",
    "    - Due to patient non-cooperation, distal right MCA branches were not well visualized.\n",
    "    - Small stenotic segment in distal left MCA.\n",
    "    - Mild stenosis in cavernous and infrapetrous parts of right ICA.\n",
    "    - Fetal-type A1 segment of right ACA noted as a normal variant.\n",
    "    \"\"\"\n",
    "\n",
    "    response = ollama.chat(model=\"mistral\", messages=[{\"role\": \"user\", \"content\": prompt}])\n",
    "    return response[\"message\"][\"content\"]\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6b372cb2-d830-4b5e-b565-c148836af3ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " - Due to patient non-cooperation, visualization of the distal right MCA branches is limited. However, a small stenotic segment in the distal left MCA and mild stenosis in both cavernous and infrapetrous parts of the right ICA are noted. The A1 segment of the right ACA was found to be of fetal type, which is considered a normal variant.\n"
     ]
    }
   ],
   "source": [
    "# Example Usage\n",
    "question = \"Patient is non-cooperative\"\n",
    "answer = \"\"\"are normal.  Rest of the cerebral parenchyma is normal.   \n",
    "Midline  structures  and corpus  callosum  are normal.   \n",
    "No haemorrhagic pathology. No extraaxial collection seen.  \n",
    "MR Angiography shows : (Patient is not cooperative)   \n",
    "Distal right MCA branches were not well visualized.   \n",
    "Small  stenotic  segment  in distal  left MCA.   \n",
    "Mild  stenosis  seen in cavernous  and infrapetrous  parts  of right  ICA.         \n",
    "Fetal  type of A1 segment of right ACA  -- Normal  varient.   \n",
    "Hypoplastic right\"\"\"\n",
    "\n",
    "summary = summarize_medical_report(question, answer)\n",
    "print(summary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b2e988d-ba62-422b-aeba-ba5e0674a14e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
